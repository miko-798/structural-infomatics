{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Q8 Classification\n",
    "\n",
    "500 epochs, lr=0.001, 3conv1dense: train acc: 52.4%; peak test acc: 41.2%; final test acc: 41.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "import cullpdb_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_RESIDUES = 700  # per protein\n",
    "RESIDUE_SIZE = 22   # size of one hot vector per residue\n",
    "NUM_CLASSES = 9     # 8 + 'NoSeq'\n",
    "\n",
    "FILTERS = 1    # Dimensions output by conv layer\n",
    "                  # equivalently, size of output by each convolutional layer\n",
    "WINDOW_SIZE = 11  # \"scope\" of convolution (ie 11 total residues)\n",
    "\n",
    "TWO_D = True\n",
    "if TWO_D:\n",
    "    INPUT_SHAPE = (NUM_RESIDUES, RESIDUE_SIZE)  # see below\n",
    "else:\n",
    "    INPUT_SHAPE = (NUM_RESIDUES * RESIDUE_SIZE,)\n",
    "OUTPUT_SIZE = NUM_CLASSES*NUM_RESIDUES      # output matrix holding predictions\n",
    "#OUTPUT_SIZE = NUM_CLASSES\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0  # momentum coefficient\n",
    "EPOCHS = 500  # iterations of training, total dataset covered once each epoch\n",
    "LOSS='categorical_crossentropy'\n",
    "OPTIMIZER = optimizers.SGD(lr=LEARNING_RATE, momentum=GAMMA, nesterov=False)\n",
    "\n",
    "SHOW_ACCURACY = True  # set to False for quicker train ops\n",
    "\n",
    "SAVE_FILE = \"models/conv-filtered-4-01\"\n",
    "DATA = \"data/cullpdb+profile_6133.npy.gz\"\n",
    "DATA_FILTERED = \"data/cullpdb+profile_6133_filtered.npy.gz\"\n",
    "DATA_TEST = \"data/cb513+profile_split1.npy.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "<b>Data:</b><br> _x represents input proteins, _y represents target structure classifications (each as one-hot vectors) <br><br>\n",
    "<b>Data Shape:</b><br> First dimension represents number of proteins, second number of residues per protein, and third size of residue or structure vector.<br> For example, train_x is shape (5600, 700, 22): it is an <b>m \\* n \\* p</b> matrix where there are <b>m</b> proteins (each row), <b>n</b> residues per protein (each column), and <b>p</b> sized vectors to represent a single residue or a single structure (each \"slice\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein residues and labels...\n",
      "Loading file from C:\\SANJAY\\CS\\Projects\\sdscbio\\data\\cullpdb+profile_6133_filtered.npy.gz...\n",
      "File Loaded.\n",
      "Loaded protein residues and labels.\n",
      "Reshaping...\n",
      "Reshaped\n",
      "Loading file from C:\\SANJAY\\CS\\Projects\\sdscbio\\data\\cb513+profile_split1.npy.gz...\n",
      "File Loaded.\n",
      "Residues:\n",
      "FDYQTVYFANQYGLRTIELGESEFVDNTLDNQHKXVIKAAWGGGYTNRNNVVINFKVDESLCDNLYFKDTDQPLVPXPASYYTLASDRIAIPKGQIXAGVEVQLTDDFFADEKSISENYVIPLLXTNVQGADSILQGKPVVENPVLTNAGDWSILPQNFVLYAVKYVNPWHGEYLRRGIDHATVAGTSKDIIRHEQFVENDEVVNISTKSXKDNLLTLKTKDESGKDISYTVRLSFAEDGSCTVHSGSQNVVVSGSGKFVSKGEKNSLGGKDRNAIYLDYTVNLTDNNIQLATKDTLVLRTRNVYGGKSLEVVRK-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Labels:\n",
      "LLLLEEELSLSEEEEEEELSLLSSSLLHHHHTTEEEEEEEEESSSSLLSLEEEEEEELGGGGTTLEETTTLLBLEELLGGGEEESLSEEEELTTLSEEEEEEEELHHHHHSGGGGSSLEEEEEEEEEEESSSEELLLEESSSSLLTTLGGGEEELLLSEEEEEEEEELTTLEEEEEEEEEEEEETTEEEEEEELLSSGGGSEEEEEEESSSSEEEEEEEEELTTSLEEEEEEEEEELTTSEEEEEELSTTLEEEEEEEEEEEEETTLGGGSLEEEEEEEEEEEETTTTEEEEEEEEEEEEELLLLSEEELLEELL-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "train_x: (5534, 700, 22)\n",
      "train_y (5534, 6300)\n",
      "test_x: (514, 700, 22)\n",
      "test_y: (514, 6300)\n"
     ]
    }
   ],
   "source": [
    "train = cullpdb_loader.load_residues(DATA_FILTERED, split=False, two_d=TWO_D)  # load from my helper file\n",
    "\n",
    "test = cullpdb_loader.load_cb513(DATA_TEST, two_d=TWO_D)\n",
    "\n",
    "# train, validation, and test were loaded as tuples of (input, output);\n",
    "# here we unpack them into separate variables:\n",
    "train_x, train_y = train\n",
    "#train_x, train_y = train_x[:2], train_y[:2]\n",
    "#vali_x, vali_y = validation\n",
    "test_x, test_y = test\n",
    "#test_x, test_y = test_x[:20], test_y[:20]\n",
    "\n",
    "cullpdb_loader.print_residues(train_x[0], labels=train_y[0], two_d=TWO_D)\n",
    "\n",
    "train_y = train_y.reshape(len(train_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "test_y = test_y.reshape(len(test_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "\n",
    "\n",
    "# print to verify data was loaded in correct shapes:\n",
    "print(\"train_x:\", train_x.shape)\n",
    "print(\"train_y\", train_y.shape)\n",
    "#print(vali_x.shape)\n",
    "#print(vali_y.shape)\n",
    "print(\"test_x:\", test_x.shape)\n",
    "print(\"test_y:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accuracy Metric (Test)\n",
    "\n",
    "Currently, if inputs and outputs are both 2D (samples, protein length, num residues/classes) and the final layer of model is Reshape (from flattened to 2D), then Keras does not compute correct loss (wrong default axis maybe?). If inputs and outputs are both 1D (samples, length * residues), then Keras calculates loss correctly (sort of) but cannot compute accuracy (no notion of separate class one-hot vectors since they are all flattened).\n",
    "<br><br>\n",
    "Working fix: Omit Reshape layer, use default Keras loss, reshape labels to 1D, and define custom accuracy to reshape outputs to 2D before comparing to predicted labels.\n",
    "<br><br>\n",
    "TODO: Define custom Loss function to calculate correct loss on 2d inputs/outputs, use default accuracy, add Reshape layer to end of model, and don't have to reshape any inputs or labels manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# old metric - accuracy inflated with trailing 'NoSeq' equality\n",
    "def custom_acc_old(obs, pred):\n",
    "    # -1 = placeholder for whatever's left\n",
    "    obs1 = K.reshape(obs, [-1, 700, 9])\n",
    "    pred1 = K.reshape(pred, [-1, 700, 9])\n",
    "    return K.mean(K.cast(\n",
    "            K.equal(K.argmax(obs1, axis=2), K.argmax(pred1, axis=2)),\n",
    "            dtype=K.floatx()))\n",
    "    \n",
    "# Second custom accuracy: ignore trailing noseq's\n",
    "def custom_acc(true, obs):\n",
    "    print(\"Using custom accuracy\")\n",
    "    if not SHOW_ACCURACY:\n",
    "        return K.constant(float('NaN'))\n",
    "    \n",
    "    # -1 = placeholder for whatever's left\n",
    "    obs = K.reshape(obs, [-1, 700, 9])\n",
    "    true = K.reshape(true, [-1, 700, 9])\n",
    "    \n",
    "    # convert one-hot vectors for residues to scalars\n",
    "    true_vals = K.argmax(true, axis=2)\n",
    "    obs_vals = K.argmax(obs, axis=2)\n",
    "    \n",
    "    # mask is 2D matrix with 1s in indices that are residues\n",
    "    # and 0s in indices that are 'NoSeq'\n",
    "    # subtract all 8's to shift 'NoSeq' values to zero\n",
    "    mask = K.sign(K.abs(true_vals - 8*K.ones_like(true_vals, dtype='int64')))\n",
    "    mask = K.cast(mask, dtype=K.floatx())\n",
    "    # 1D vector with each index the number of non-'NoSeq' residues \n",
    "    # in corresponding protein\n",
    "    length = K.sum(mask, axis=1)\n",
    "    \n",
    "    # compare observed and predicted values (cast from boolean to 1s and 0s),\n",
    "    # then multiply by mask to nullify any trailing 'NoSeq' equalities\n",
    "    comparison = K.cast(K.equal(true_vals, obs_vals), dtype=K.floatx())\n",
    "    comparison = comparison * mask\n",
    "    \n",
    "    # and return average\n",
    "    return K.sum(comparison) / K.sum(length)\n",
    "    \n",
    "\n",
    "# not working\n",
    "def custom_loss(obs, pred):\n",
    "    pred = tf.Variable(pred)\n",
    "    pred.set_shape([None, NUM_RESIDUES, NUM_CLASSES])\n",
    "    obs = tf.Variable(obs)\n",
    "    obs.set_shape([None, NUM_RESIDUES, NUM_CLASSES])\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=pred, logits=obs, name='custom-loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "The model is constructed to currently have 3 convolutional layers.<br><br>\n",
    "A convolutional window \"slides\" across the input, each time taking the nearest <i>WINDOW_SIZE</i> number of features (residues) as inputs and outputing a single vector of dimension <i>FILTERS</i>.\n",
    "<br><br>\n",
    "This 3d data (samples, Protein Length, <i>FILTER_SIZE</i>) is flattened to 2d (samples, Length \\* <i>FILTER_SIZE</i>). Finally a \"dense\", fully connected layer reduces the dimensionality of the data from the previous <i>FILTERS</i> dimensions to <i>OUTPUT_SIZE</i> dimensions (here 9, 8 for each secondary structure class and 1 for None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# border_mode indicates how the convolution handles the edges of the input (where the window \"sticks out\").\n",
    "# The 'same' setting pads the inputs with zeros on either side.\n",
    "# Only the first layer requires an input_shape parameter; successive layers infer it from within the model.\n",
    "\n",
    "# 1st layer\n",
    "model.add(Convolution1D(\n",
    "        FILTERS, WINDOW_SIZE, activation='tanh', border_mode='same', input_shape=INPUT_SHAPE))\n",
    "# 2nd layer\n",
    "model.add(Convolution1D(\n",
    "        FILTERS, WINDOW_SIZE, activation='tanh', border_mode='same'))\n",
    "# 3rd layer\n",
    "model.add(Convolution1D(\n",
    "        FILTERS, WINDOW_SIZE, activation='tanh', border_mode='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# 'lecun_uniform' indicates that the weights should be initialized to small random values in a certain normal distribution.\n",
    "model.add(Dense(OUTPUT_SIZE, init='lecun_uniform', name=\"OutputLayer\", activation='softmax'))\n",
    "#model.add(Reshape((NUM_RESIDUES, NUM_CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model given a loss function, optimizer, and learning rate (specified above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom accuracy\n"
     ]
    }
   ],
   "source": [
    "# optimizer= takes either string or optimizer object\n",
    "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[custom_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on training data against target training labels, show accuracy on validation data each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5534 samples, validate on 514 samples\n",
      "Epoch 1/500\n",
      "18s - loss: 5266.6308 - custom_acc: 0.1788 - val_loss: 5038.7362 - val_custom_acc: 0.2080\n",
      "Epoch 2/500\n",
      "18s - loss: 5092.9751 - custom_acc: 0.2256 - val_loss: 4981.1403 - val_custom_acc: 0.2475\n",
      "Epoch 3/500\n",
      "18s - loss: 5050.5828 - custom_acc: 0.2736 - val_loss: 4963.8045 - val_custom_acc: 0.2893\n",
      "Epoch 4/500\n",
      "18s - loss: 5022.7868 - custom_acc: 0.3019 - val_loss: 4937.3310 - val_custom_acc: 0.2732\n",
      "Epoch 5/500\n",
      "18s - loss: 5005.9754 - custom_acc: 0.3140 - val_loss: 4925.7658 - val_custom_acc: 0.2865\n",
      "Epoch 6/500\n",
      "18s - loss: 4994.5534 - custom_acc: 0.3220 - val_loss: 4918.4452 - val_custom_acc: 0.2927\n",
      "Epoch 7/500\n",
      "18s - loss: 4986.5972 - custom_acc: 0.3277 - val_loss: 4913.0527 - val_custom_acc: 0.2961\n",
      "Epoch 8/500\n",
      "18s - loss: 4980.6696 - custom_acc: 0.3314 - val_loss: 4908.8611 - val_custom_acc: 0.2982\n",
      "Epoch 9/500\n",
      "17s - loss: 4976.0326 - custom_acc: 0.3343 - val_loss: 4905.4896 - val_custom_acc: 0.2998\n",
      "Epoch 10/500\n",
      "17s - loss: 4972.2647 - custom_acc: 0.3366 - val_loss: 4902.7214 - val_custom_acc: 0.3008\n",
      "Epoch 11/500\n",
      "17s - loss: 4969.1669 - custom_acc: 0.3386 - val_loss: 4900.3980 - val_custom_acc: 0.3018\n",
      "Epoch 12/500\n",
      "17s - loss: 4966.5700 - custom_acc: 0.3401 - val_loss: 4898.4249 - val_custom_acc: 0.3030\n",
      "Epoch 13/500\n",
      "18s - loss: 4964.3619 - custom_acc: 0.3413 - val_loss: 4896.7236 - val_custom_acc: 0.3035\n",
      "Epoch 14/500\n",
      "17s - loss: 4962.4540 - custom_acc: 0.3425 - val_loss: 4895.2446 - val_custom_acc: 0.3040\n",
      "Epoch 15/500\n",
      "18s - loss: 4960.7874 - custom_acc: 0.3435 - val_loss: 4893.9420 - val_custom_acc: 0.3042\n",
      "Epoch 16/500\n",
      "17s - loss: 4959.3142 - custom_acc: 0.3443 - val_loss: 4892.7857 - val_custom_acc: 0.3046\n",
      "Epoch 17/500\n",
      "18s - loss: 4958.0004 - custom_acc: 0.3450 - val_loss: 4891.7533 - val_custom_acc: 0.3048\n",
      "Epoch 18/500\n",
      "18s - loss: 4956.8181 - custom_acc: 0.3458 - val_loss: 4890.8256 - val_custom_acc: 0.3051\n",
      "Epoch 19/500\n",
      "18s - loss: 4955.7465 - custom_acc: 0.3464 - val_loss: 4889.9870 - val_custom_acc: 0.3059\n",
      "Epoch 20/500\n",
      "18s - loss: 4954.7692 - custom_acc: 0.3469 - val_loss: 4889.2262 - val_custom_acc: 0.3060\n",
      "Epoch 21/500\n",
      "18s - loss: 4953.8739 - custom_acc: 0.3474 - val_loss: 4888.5330 - val_custom_acc: 0.3061\n",
      "Epoch 22/500\n",
      "18s - loss: 4953.0494 - custom_acc: 0.3478 - val_loss: 4887.8983 - val_custom_acc: 0.3065\n",
      "Epoch 23/500\n",
      "18s - loss: 4952.2878 - custom_acc: 0.3482 - val_loss: 4887.3153 - val_custom_acc: 0.3066\n",
      "Epoch 24/500\n",
      "17s - loss: 4951.5819 - custom_acc: 0.3486 - val_loss: 4886.7781 - val_custom_acc: 0.3066\n",
      "Epoch 25/500\n",
      "18s - loss: 4950.9258 - custom_acc: 0.3490 - val_loss: 4886.2804 - val_custom_acc: 0.3070\n",
      "Epoch 26/500\n",
      "18s - loss: 4950.3140 - custom_acc: 0.3492 - val_loss: 4885.8180 - val_custom_acc: 0.3074\n",
      "Epoch 27/500\n",
      "18s - loss: 4949.7417 - custom_acc: 0.3495 - val_loss: 4885.3875 - val_custom_acc: 0.3074\n",
      "Epoch 28/500\n",
      "17s - loss: 4949.2050 - custom_acc: 0.3498 - val_loss: 4884.9845 - val_custom_acc: 0.3074\n",
      "Epoch 29/500\n",
      "17s - loss: 4948.7002 - custom_acc: 0.3502 - val_loss: 4884.6067 - val_custom_acc: 0.3076\n",
      "Epoch 30/500\n",
      "18s - loss: 4948.2238 - custom_acc: 0.3504 - val_loss: 4884.2509 - val_custom_acc: 0.3076\n",
      "Epoch 31/500\n",
      "17s - loss: 4947.7732 - custom_acc: 0.3507 - val_loss: 4883.9152 - val_custom_acc: 0.3080\n",
      "Epoch 32/500\n",
      "17s - loss: 4947.3457 - custom_acc: 0.3509 - val_loss: 4883.5971 - val_custom_acc: 0.3080\n",
      "Epoch 33/500\n",
      "18s - loss: 4946.9391 - custom_acc: 0.3511 - val_loss: 4883.2947 - val_custom_acc: 0.3082\n",
      "Epoch 34/500\n",
      "18s - loss: 4946.5514 - custom_acc: 0.3514 - val_loss: 4883.0072 - val_custom_acc: 0.3083\n",
      "Epoch 35/500\n",
      "18s - loss: 4946.1808 - custom_acc: 0.3517 - val_loss: 4882.7320 - val_custom_acc: 0.3086\n",
      "Epoch 36/500\n",
      "18s - loss: 4945.8258 - custom_acc: 0.3519 - val_loss: 4882.4686 - val_custom_acc: 0.3086\n",
      "Epoch 37/500\n",
      "18s - loss: 4945.4848 - custom_acc: 0.3521 - val_loss: 4882.2157 - val_custom_acc: 0.3088\n",
      "Epoch 38/500\n",
      "20s - loss: 4945.1567 - custom_acc: 0.3524 - val_loss: 4881.9720 - val_custom_acc: 0.3090\n",
      "Epoch 39/500\n",
      "20s - loss: 4944.8402 - custom_acc: 0.3527 - val_loss: 4881.7364 - val_custom_acc: 0.3090\n",
      "Epoch 40/500\n",
      "20s - loss: 4944.5345 - custom_acc: 0.3530 - val_loss: 4881.5090 - val_custom_acc: 0.3089\n",
      "Epoch 41/500\n",
      "20s - loss: 4944.2382 - custom_acc: 0.3532 - val_loss: 4881.2879 - val_custom_acc: 0.3088\n",
      "Epoch 42/500\n",
      "20s - loss: 4943.9507 - custom_acc: 0.3535 - val_loss: 4881.0728 - val_custom_acc: 0.3090\n",
      "Epoch 43/500\n",
      "20s - loss: 4943.6712 - custom_acc: 0.3538 - val_loss: 4880.8630 - val_custom_acc: 0.3094\n",
      "Epoch 44/500\n",
      "20s - loss: 4943.3986 - custom_acc: 0.3540 - val_loss: 4880.6578 - val_custom_acc: 0.3098\n",
      "Epoch 45/500\n",
      "20s - loss: 4943.1324 - custom_acc: 0.3544 - val_loss: 4880.4563 - val_custom_acc: 0.3101\n",
      "Epoch 46/500\n",
      "20s - loss: 4942.8718 - custom_acc: 0.3547 - val_loss: 4880.2581 - val_custom_acc: 0.3105\n",
      "Epoch 47/500\n",
      "20s - loss: 4942.6160 - custom_acc: 0.3550 - val_loss: 4880.0621 - val_custom_acc: 0.3108\n",
      "Epoch 48/500\n",
      "20s - loss: 4942.3644 - custom_acc: 0.3553 - val_loss: 4879.8683 - val_custom_acc: 0.3111\n",
      "Epoch 49/500\n",
      "20s - loss: 4942.1162 - custom_acc: 0.3557 - val_loss: 4879.6755 - val_custom_acc: 0.3113\n",
      "Epoch 50/500\n",
      "20s - loss: 4941.8707 - custom_acc: 0.3561 - val_loss: 4879.4827 - val_custom_acc: 0.3115\n",
      "Epoch 51/500\n",
      "20s - loss: 4941.6274 - custom_acc: 0.3564 - val_loss: 4879.2894 - val_custom_acc: 0.3117\n",
      "Epoch 52/500\n",
      "18s - loss: 4941.3860 - custom_acc: 0.3568 - val_loss: 4879.0947 - val_custom_acc: 0.3121\n",
      "Epoch 53/500\n",
      "17s - loss: 4941.1472 - custom_acc: 0.3573 - val_loss: 4878.8991 - val_custom_acc: 0.3125\n",
      "Epoch 54/500\n",
      "17s - loss: 4940.9114 - custom_acc: 0.3577 - val_loss: 4878.7029 - val_custom_acc: 0.3129\n",
      "Epoch 55/500\n",
      "17s - loss: 4940.6792 - custom_acc: 0.3583 - val_loss: 4878.5076 - val_custom_acc: 0.3134\n",
      "Epoch 56/500\n",
      "17s - loss: 4940.4510 - custom_acc: 0.3587 - val_loss: 4878.3136 - val_custom_acc: 0.3140\n",
      "Epoch 57/500\n",
      "17s - loss: 4940.2260 - custom_acc: 0.3592 - val_loss: 4878.1207 - val_custom_acc: 0.3146\n",
      "Epoch 58/500\n",
      "17s - loss: 4940.0041 - custom_acc: 0.3597 - val_loss: 4877.9283 - val_custom_acc: 0.3148\n",
      "Epoch 59/500\n",
      "17s - loss: 4939.7864 - custom_acc: 0.3602 - val_loss: 4877.7363 - val_custom_acc: 0.3154\n",
      "Epoch 60/500\n",
      "17s - loss: 4939.5694 - custom_acc: 0.3606 - val_loss: 4877.5440 - val_custom_acc: 0.3161\n",
      "Epoch 61/500\n",
      "17s - loss: 4939.3535 - custom_acc: 0.3611 - val_loss: 4877.3531 - val_custom_acc: 0.3169\n",
      "Epoch 62/500\n",
      "18s - loss: 4939.1388 - custom_acc: 0.3615 - val_loss: 4877.1627 - val_custom_acc: 0.3170\n",
      "Epoch 63/500\n",
      "18s - loss: 4938.9241 - custom_acc: 0.3620 - val_loss: 4876.9754 - val_custom_acc: 0.3175\n",
      "Epoch 64/500\n",
      "18s - loss: 4938.7137 - custom_acc: 0.3625 - val_loss: 4876.7926 - val_custom_acc: 0.3180\n",
      "Epoch 65/500\n",
      "17s - loss: 4938.5036 - custom_acc: 0.3629 - val_loss: 4876.6131 - val_custom_acc: 0.3187\n",
      "Epoch 66/500\n",
      "18s - loss: 4938.2965 - custom_acc: 0.3633 - val_loss: 4876.4392 - val_custom_acc: 0.3191\n",
      "Epoch 67/500\n",
      "17s - loss: 4938.0981 - custom_acc: 0.3636 - val_loss: 4876.2732 - val_custom_acc: 0.3195\n",
      "Epoch 68/500\n",
      "17s - loss: 4937.8980 - custom_acc: 0.3641 - val_loss: 4876.1119 - val_custom_acc: 0.3197\n",
      "Epoch 69/500\n",
      "17s - loss: 4937.6924 - custom_acc: 0.3645 - val_loss: 4875.9572 - val_custom_acc: 0.3204\n",
      "Epoch 70/500\n",
      "18s - loss: 4937.5049 - custom_acc: 0.3648 - val_loss: 4875.8104 - val_custom_acc: 0.3206\n",
      "Epoch 71/500\n",
      "18s - loss: 4937.3013 - custom_acc: 0.3652 - val_loss: 4875.6694 - val_custom_acc: 0.3209\n",
      "Epoch 72/500\n",
      "17s - loss: 4937.1122 - custom_acc: 0.3655 - val_loss: 4875.5355 - val_custom_acc: 0.3211\n",
      "Epoch 73/500\n",
      "17s - loss: 4936.9359 - custom_acc: 0.3658 - val_loss: 4875.4078 - val_custom_acc: 0.3212\n",
      "Epoch 74/500\n",
      "17s - loss: 4936.7322 - custom_acc: 0.3662 - val_loss: 4875.2837 - val_custom_acc: 0.3217\n",
      "Epoch 75/500\n",
      "18s - loss: 4936.5553 - custom_acc: 0.3665 - val_loss: 4875.1653 - val_custom_acc: 0.3220\n",
      "Epoch 76/500\n",
      "17s - loss: 4936.3708 - custom_acc: 0.3668 - val_loss: 4875.0506 - val_custom_acc: 0.3220\n",
      "Epoch 77/500\n",
      "17s - loss: 4936.1992 - custom_acc: 0.3671 - val_loss: 4874.9400 - val_custom_acc: 0.3224\n",
      "Epoch 78/500\n",
      "18s - loss: 4936.0230 - custom_acc: 0.3674 - val_loss: 4874.8319 - val_custom_acc: 0.3228\n",
      "Epoch 79/500\n",
      "18s - loss: 4935.8576 - custom_acc: 0.3676 - val_loss: 4874.7279 - val_custom_acc: 0.3231\n",
      "Epoch 80/500\n",
      "17s - loss: 4935.6867 - custom_acc: 0.3679 - val_loss: 4874.6255 - val_custom_acc: 0.3233\n",
      "Epoch 81/500\n",
      "17s - loss: 4935.5225 - custom_acc: 0.3682 - val_loss: 4874.5252 - val_custom_acc: 0.3233\n",
      "Epoch 82/500\n",
      "17s - loss: 4935.3623 - custom_acc: 0.3686 - val_loss: 4874.4278 - val_custom_acc: 0.3234\n",
      "Epoch 83/500\n",
      "18s - loss: 4935.1929 - custom_acc: 0.3689 - val_loss: 4874.3320 - val_custom_acc: 0.3236\n",
      "Epoch 84/500\n",
      "18s - loss: 4935.0384 - custom_acc: 0.3692 - val_loss: 4874.2372 - val_custom_acc: 0.3238\n",
      "Epoch 85/500\n",
      "17s - loss: 4934.8883 - custom_acc: 0.3695 - val_loss: 4874.1446 - val_custom_acc: 0.3240\n",
      "Epoch 86/500\n",
      "17s - loss: 4934.7369 - custom_acc: 0.3698 - val_loss: 4874.0533 - val_custom_acc: 0.3240\n",
      "Epoch 87/500\n",
      "17s - loss: 4934.5842 - custom_acc: 0.3701 - val_loss: 4873.9625 - val_custom_acc: 0.3242\n",
      "Epoch 88/500\n",
      "18s - loss: 4934.4344 - custom_acc: 0.3704 - val_loss: 4873.8735 - val_custom_acc: 0.3242\n",
      "Epoch 89/500\n",
      "17s - loss: 4934.2893 - custom_acc: 0.3707 - val_loss: 4873.7853 - val_custom_acc: 0.3243\n",
      "Epoch 90/500\n",
      "18s - loss: 4934.1524 - custom_acc: 0.3710 - val_loss: 4873.6981 - val_custom_acc: 0.3246\n",
      "Epoch 91/500\n",
      "18s - loss: 4933.9970 - custom_acc: 0.3713 - val_loss: 4873.6134 - val_custom_acc: 0.3248\n",
      "Epoch 92/500\n",
      "17s - loss: 4933.8631 - custom_acc: 0.3716 - val_loss: 4873.5285 - val_custom_acc: 0.3247\n",
      "Epoch 93/500\n",
      "18s - loss: 4933.7234 - custom_acc: 0.3718 - val_loss: 4873.4458 - val_custom_acc: 0.3249\n",
      "Epoch 94/500\n",
      "18s - loss: 4933.5874 - custom_acc: 0.3721 - val_loss: 4873.3623 - val_custom_acc: 0.3251\n",
      "Epoch 95/500\n",
      "18s - loss: 4933.4524 - custom_acc: 0.3725 - val_loss: 4873.2847 - val_custom_acc: 0.3252\n",
      "Epoch 96/500\n",
      "18s - loss: 4933.3205 - custom_acc: 0.3728 - val_loss: 4873.2064 - val_custom_acc: 0.3254\n",
      "Epoch 97/500\n",
      "17s - loss: 4933.1956 - custom_acc: 0.3731 - val_loss: 4873.1303 - val_custom_acc: 0.3257\n",
      "Epoch 98/500\n",
      "17s - loss: 4933.0675 - custom_acc: 0.3734 - val_loss: 4873.0565 - val_custom_acc: 0.3262\n",
      "Epoch 99/500\n",
      "18s - loss: 4932.9432 - custom_acc: 0.3736 - val_loss: 4872.9852 - val_custom_acc: 0.3265\n",
      "Epoch 100/500\n",
      "17s - loss: 4932.8285 - custom_acc: 0.3739 - val_loss: 4872.9149 - val_custom_acc: 0.3268\n",
      "Epoch 101/500\n",
      "17s - loss: 4932.6995 - custom_acc: 0.3742 - val_loss: 4872.8485 - val_custom_acc: 0.3273\n",
      "Epoch 102/500\n",
      "18s - loss: 4932.5833 - custom_acc: 0.3745 - val_loss: 4872.7832 - val_custom_acc: 0.3275\n",
      "Epoch 103/500\n",
      "18s - loss: 4932.4616 - custom_acc: 0.3747 - val_loss: 4872.7164 - val_custom_acc: 0.3279\n",
      "Epoch 104/500\n",
      "17s - loss: 4932.3555 - custom_acc: 0.3750 - val_loss: 4872.6557 - val_custom_acc: 0.3283\n",
      "Epoch 105/500\n",
      "17s - loss: 4932.2420 - custom_acc: 0.3753 - val_loss: 4872.5946 - val_custom_acc: 0.3285\n",
      "Epoch 106/500\n",
      "17s - loss: 4932.1325 - custom_acc: 0.3756 - val_loss: 4872.5349 - val_custom_acc: 0.3287\n",
      "Epoch 107/500\n",
      "17s - loss: 4932.0248 - custom_acc: 0.3759 - val_loss: 4872.4760 - val_custom_acc: 0.3286\n",
      "Epoch 108/500\n",
      "17s - loss: 4931.9155 - custom_acc: 0.3762 - val_loss: 4872.4167 - val_custom_acc: 0.3287\n",
      "Epoch 109/500\n",
      "17s - loss: 4931.8102 - custom_acc: 0.3765 - val_loss: 4872.3628 - val_custom_acc: 0.3291\n",
      "Epoch 110/500\n",
      "17s - loss: 4931.7074 - custom_acc: 0.3767 - val_loss: 4872.3061 - val_custom_acc: 0.3293\n",
      "Epoch 111/500\n",
      "17s - loss: 4931.6074 - custom_acc: 0.3770 - val_loss: 4872.2491 - val_custom_acc: 0.3293\n",
      "Epoch 112/500\n",
      "17s - loss: 4931.5037 - custom_acc: 0.3772 - val_loss: 4872.1973 - val_custom_acc: 0.3296\n",
      "Epoch 113/500\n",
      "17s - loss: 4931.4034 - custom_acc: 0.3775 - val_loss: 4872.1441 - val_custom_acc: 0.3299\n",
      "Epoch 114/500\n",
      "17s - loss: 4931.3093 - custom_acc: 0.3777 - val_loss: 4872.0891 - val_custom_acc: 0.3301\n",
      "Epoch 115/500\n",
      "17s - loss: 4931.2110 - custom_acc: 0.3780 - val_loss: 4872.0368 - val_custom_acc: 0.3303\n",
      "Epoch 116/500\n",
      "18s - loss: 4931.1125 - custom_acc: 0.3783 - val_loss: 4871.9861 - val_custom_acc: 0.3306\n",
      "Epoch 117/500\n",
      "17s - loss: 4931.0181 - custom_acc: 0.3786 - val_loss: 4871.9339 - val_custom_acc: 0.3309\n",
      "Epoch 118/500\n",
      "17s - loss: 4930.9247 - custom_acc: 0.3789 - val_loss: 4871.8827 - val_custom_acc: 0.3314\n",
      "Epoch 119/500\n",
      "17s - loss: 4930.8320 - custom_acc: 0.3791 - val_loss: 4871.8334 - val_custom_acc: 0.3315\n",
      "Epoch 120/500\n",
      "17s - loss: 4930.7411 - custom_acc: 0.3794 - val_loss: 4871.7781 - val_custom_acc: 0.3319\n",
      "Epoch 121/500\n",
      "18s - loss: 4930.6494 - custom_acc: 0.3796 - val_loss: 4871.7326 - val_custom_acc: 0.3320\n",
      "Epoch 122/500\n",
      "17s - loss: 4930.5600 - custom_acc: 0.3799 - val_loss: 4871.6795 - val_custom_acc: 0.3322\n",
      "Epoch 123/500\n",
      "17s - loss: 4930.4686 - custom_acc: 0.3802 - val_loss: 4871.6342 - val_custom_acc: 0.3324\n",
      "Epoch 124/500\n",
      "17s - loss: 4930.3799 - custom_acc: 0.3805 - val_loss: 4871.5851 - val_custom_acc: 0.3326\n",
      "Epoch 125/500\n",
      "18s - loss: 4930.2938 - custom_acc: 0.3807 - val_loss: 4871.5367 - val_custom_acc: 0.3328\n",
      "Epoch 126/500\n",
      "17s - loss: 4930.2113 - custom_acc: 0.3810 - val_loss: 4871.4896 - val_custom_acc: 0.3330\n",
      "Epoch 127/500\n",
      "17s - loss: 4930.1194 - custom_acc: 0.3812 - val_loss: 4871.4374 - val_custom_acc: 0.3331\n",
      "Epoch 128/500\n",
      "18s - loss: 4930.0361 - custom_acc: 0.3815 - val_loss: 4871.3976 - val_custom_acc: 0.3332\n",
      "Epoch 129/500\n",
      "17s - loss: 4929.9575 - custom_acc: 0.3818 - val_loss: 4871.3507 - val_custom_acc: 0.3334\n",
      "Epoch 130/500\n",
      "18s - loss: 4929.8691 - custom_acc: 0.3820 - val_loss: 4871.3065 - val_custom_acc: 0.3339\n",
      "Epoch 131/500\n",
      "18s - loss: 4929.7837 - custom_acc: 0.3823 - val_loss: 4871.2574 - val_custom_acc: 0.3342\n",
      "Epoch 132/500\n",
      "17s - loss: 4929.7047 - custom_acc: 0.3826 - val_loss: 4871.2169 - val_custom_acc: 0.3343\n",
      "Epoch 133/500\n",
      "17s - loss: 4929.6212 - custom_acc: 0.3828 - val_loss: 4871.1726 - val_custom_acc: 0.3347\n",
      "Epoch 134/500\n",
      "18s - loss: 4929.5418 - custom_acc: 0.3831 - val_loss: 4871.1205 - val_custom_acc: 0.3351\n",
      "Epoch 135/500\n",
      "17s - loss: 4929.4658 - custom_acc: 0.3833 - val_loss: 4871.0835 - val_custom_acc: 0.3354\n",
      "Epoch 136/500\n",
      "17s - loss: 4929.3800 - custom_acc: 0.3836 - val_loss: 4871.0459 - val_custom_acc: 0.3355\n",
      "Epoch 137/500\n",
      "18s - loss: 4929.2997 - custom_acc: 0.3839 - val_loss: 4870.9997 - val_custom_acc: 0.3359\n",
      "Epoch 138/500\n",
      "17s - loss: 4929.2230 - custom_acc: 0.3841 - val_loss: 4870.9568 - val_custom_acc: 0.3363\n",
      "Epoch 139/500\n",
      "17s - loss: 4929.1470 - custom_acc: 0.3844 - val_loss: 4870.9213 - val_custom_acc: 0.3366\n",
      "Epoch 140/500\n",
      "17s - loss: 4929.0720 - custom_acc: 0.3846 - val_loss: 4870.8789 - val_custom_acc: 0.3368\n",
      "Epoch 141/500\n",
      "18s - loss: 4928.9957 - custom_acc: 0.3849 - val_loss: 4870.8399 - val_custom_acc: 0.3372\n",
      "Epoch 142/500\n",
      "17s - loss: 4928.9150 - custom_acc: 0.3852 - val_loss: 4870.8010 - val_custom_acc: 0.3374\n",
      "Epoch 143/500\n",
      "17s - loss: 4928.8386 - custom_acc: 0.3854 - val_loss: 4870.7622 - val_custom_acc: 0.3377\n",
      "Epoch 144/500\n",
      "18s - loss: 4928.7676 - custom_acc: 0.3857 - val_loss: 4870.7209 - val_custom_acc: 0.3378\n",
      "Epoch 145/500\n",
      "18s - loss: 4928.6789 - custom_acc: 0.3860 - val_loss: 4870.6763 - val_custom_acc: 0.3380\n",
      "Epoch 146/500\n",
      "17s - loss: 4928.6113 - custom_acc: 0.3863 - val_loss: 4870.6397 - val_custom_acc: 0.3383\n",
      "Epoch 147/500\n",
      "18s - loss: 4928.5352 - custom_acc: 0.3866 - val_loss: 4870.6064 - val_custom_acc: 0.3386\n",
      "Epoch 148/500\n",
      "17s - loss: 4928.4594 - custom_acc: 0.3869 - val_loss: 4870.5677 - val_custom_acc: 0.3388\n",
      "Epoch 149/500\n",
      "18s - loss: 4928.3884 - custom_acc: 0.3871 - val_loss: 4870.5296 - val_custom_acc: 0.3391\n",
      "Epoch 150/500\n",
      "18s - loss: 4928.3146 - custom_acc: 0.3874 - val_loss: 4870.4878 - val_custom_acc: 0.3394\n",
      "Epoch 151/500\n",
      "17s - loss: 4928.2419 - custom_acc: 0.3877 - val_loss: 4870.4512 - val_custom_acc: 0.3398\n",
      "Epoch 152/500\n",
      "17s - loss: 4928.1613 - custom_acc: 0.3881 - val_loss: 4870.4178 - val_custom_acc: 0.3401\n",
      "Epoch 153/500\n",
      "17s - loss: 4928.0890 - custom_acc: 0.3884 - val_loss: 4870.3831 - val_custom_acc: 0.3405\n",
      "Epoch 154/500\n",
      "17s - loss: 4928.0169 - custom_acc: 0.3887 - val_loss: 4870.3434 - val_custom_acc: 0.3408\n",
      "Epoch 155/500\n",
      "17s - loss: 4927.9441 - custom_acc: 0.3890 - val_loss: 4870.3077 - val_custom_acc: 0.3410\n",
      "Epoch 156/500\n",
      "17s - loss: 4927.8710 - custom_acc: 0.3894 - val_loss: 4870.2660 - val_custom_acc: 0.3415\n",
      "Epoch 157/500\n",
      "17s - loss: 4927.8034 - custom_acc: 0.3897 - val_loss: 4870.2278 - val_custom_acc: 0.3418\n",
      "Epoch 158/500\n",
      "17s - loss: 4927.7234 - custom_acc: 0.3900 - val_loss: 4870.1851 - val_custom_acc: 0.3422\n",
      "Epoch 159/500\n",
      "17s - loss: 4927.6508 - custom_acc: 0.3903 - val_loss: 4870.1596 - val_custom_acc: 0.3422\n",
      "Epoch 160/500\n",
      "18s - loss: 4927.5728 - custom_acc: 0.3907 - val_loss: 4870.1263 - val_custom_acc: 0.3425\n",
      "Epoch 161/500\n",
      "17s - loss: 4927.5044 - custom_acc: 0.3910 - val_loss: 4870.0903 - val_custom_acc: 0.3430\n",
      "Epoch 162/500\n",
      "17s - loss: 4927.4339 - custom_acc: 0.3914 - val_loss: 4870.0524 - val_custom_acc: 0.3435\n",
      "Epoch 163/500\n",
      "17s - loss: 4927.3568 - custom_acc: 0.3917 - val_loss: 4870.0168 - val_custom_acc: 0.3438\n",
      "Epoch 164/500\n",
      "17s - loss: 4927.2859 - custom_acc: 0.3920 - val_loss: 4869.9842 - val_custom_acc: 0.3443\n",
      "Epoch 165/500\n",
      "17s - loss: 4927.2090 - custom_acc: 0.3924 - val_loss: 4869.9463 - val_custom_acc: 0.3445\n",
      "Epoch 166/500\n",
      "17s - loss: 4927.1337 - custom_acc: 0.3927 - val_loss: 4869.9098 - val_custom_acc: 0.3450\n",
      "Epoch 167/500\n",
      "17s - loss: 4927.0568 - custom_acc: 0.3931 - val_loss: 4869.8733 - val_custom_acc: 0.3453\n",
      "Epoch 168/500\n",
      "17s - loss: 4926.9859 - custom_acc: 0.3935 - val_loss: 4869.8385 - val_custom_acc: 0.3455\n",
      "Epoch 169/500\n",
      "18s - loss: 4926.9078 - custom_acc: 0.3938 - val_loss: 4869.7930 - val_custom_acc: 0.3461\n",
      "Epoch 170/500\n",
      "17s - loss: 4926.8429 - custom_acc: 0.3942 - val_loss: 4869.7555 - val_custom_acc: 0.3463\n",
      "Epoch 171/500\n",
      "17s - loss: 4926.7539 - custom_acc: 0.3946 - val_loss: 4869.7320 - val_custom_acc: 0.3466\n",
      "Epoch 172/500\n",
      "17s - loss: 4926.6810 - custom_acc: 0.3948 - val_loss: 4869.6928 - val_custom_acc: 0.3469\n",
      "Epoch 173/500\n",
      "17s - loss: 4926.5973 - custom_acc: 0.3953 - val_loss: 4869.6451 - val_custom_acc: 0.3476\n",
      "Epoch 174/500\n",
      "17s - loss: 4926.5248 - custom_acc: 0.3957 - val_loss: 4869.6148 - val_custom_acc: 0.3479\n",
      "Epoch 175/500\n",
      "17s - loss: 4926.4414 - custom_acc: 0.3961 - val_loss: 4869.5777 - val_custom_acc: 0.3482\n",
      "Epoch 176/500\n",
      "17s - loss: 4926.3660 - custom_acc: 0.3965 - val_loss: 4869.5354 - val_custom_acc: 0.3488\n",
      "Epoch 177/500\n",
      "17s - loss: 4926.2808 - custom_acc: 0.3970 - val_loss: 4869.5035 - val_custom_acc: 0.3491\n",
      "Epoch 178/500\n",
      "18s - loss: 4926.1939 - custom_acc: 0.3974 - val_loss: 4869.4678 - val_custom_acc: 0.3496\n",
      "Epoch 179/500\n",
      "18s - loss: 4926.1100 - custom_acc: 0.3978 - val_loss: 4869.4372 - val_custom_acc: 0.3499\n",
      "Epoch 180/500\n",
      "17s - loss: 4926.0268 - custom_acc: 0.3983 - val_loss: 4869.4055 - val_custom_acc: 0.3504\n",
      "Epoch 181/500\n",
      "17s - loss: 4925.9435 - custom_acc: 0.3988 - val_loss: 4869.3744 - val_custom_acc: 0.3509\n",
      "Epoch 182/500\n",
      "17s - loss: 4925.8467 - custom_acc: 0.3993 - val_loss: 4869.3379 - val_custom_acc: 0.3514\n",
      "Epoch 183/500\n",
      "17s - loss: 4925.7460 - custom_acc: 0.4000 - val_loss: 4869.3052 - val_custom_acc: 0.3519\n",
      "Epoch 184/500\n",
      "18s - loss: 4925.6496 - custom_acc: 0.4005 - val_loss: 4869.2833 - val_custom_acc: 0.3520\n",
      "Epoch 185/500\n",
      "17s - loss: 4925.5684 - custom_acc: 0.4009 - val_loss: 4869.2563 - val_custom_acc: 0.3524\n",
      "Epoch 186/500\n",
      "18s - loss: 4925.4856 - custom_acc: 0.4013 - val_loss: 4869.2213 - val_custom_acc: 0.3526\n",
      "Epoch 187/500\n",
      "18s - loss: 4925.3808 - custom_acc: 0.4019 - val_loss: 4869.1976 - val_custom_acc: 0.3532\n",
      "Epoch 188/500\n",
      "18s - loss: 4925.2927 - custom_acc: 0.4025 - val_loss: 4869.1883 - val_custom_acc: 0.3535\n",
      "Epoch 189/500\n",
      "18s - loss: 4925.1997 - custom_acc: 0.4030 - val_loss: 4869.1554 - val_custom_acc: 0.3538\n",
      "Epoch 190/500\n",
      "18s - loss: 4925.1076 - custom_acc: 0.4036 - val_loss: 4869.1532 - val_custom_acc: 0.3544\n",
      "Epoch 191/500\n",
      "18s - loss: 4925.0006 - custom_acc: 0.4042 - val_loss: 4869.1207 - val_custom_acc: 0.3551\n",
      "Epoch 192/500\n",
      "17s - loss: 4924.8928 - custom_acc: 0.4048 - val_loss: 4869.1195 - val_custom_acc: 0.3552\n",
      "Epoch 193/500\n",
      "17s - loss: 4924.7852 - custom_acc: 0.4055 - val_loss: 4869.1447 - val_custom_acc: 0.3555\n",
      "Epoch 194/500\n",
      "17s - loss: 4924.6602 - custom_acc: 0.4063 - val_loss: 4869.2022 - val_custom_acc: 0.3559\n",
      "Epoch 195/500\n",
      "17s - loss: 4924.5536 - custom_acc: 0.4069 - val_loss: 4869.2978 - val_custom_acc: 0.3558\n",
      "Epoch 196/500\n",
      "18s - loss: 4924.4652 - custom_acc: 0.4074 - val_loss: 4869.2750 - val_custom_acc: 0.3562\n",
      "Epoch 197/500\n",
      "17s - loss: 4924.3449 - custom_acc: 0.4081 - val_loss: 4869.3250 - val_custom_acc: 0.3565\n",
      "Epoch 198/500\n",
      "17s - loss: 4924.2547 - custom_acc: 0.4085 - val_loss: 4869.3446 - val_custom_acc: 0.3566\n",
      "Epoch 199/500\n",
      "17s - loss: 4924.2641 - custom_acc: 0.4087 - val_loss: 4869.1168 - val_custom_acc: 0.3578\n",
      "Epoch 200/500\n",
      "17s - loss: 4924.0463 - custom_acc: 0.4098 - val_loss: 4869.0325 - val_custom_acc: 0.3587\n",
      "Epoch 201/500\n",
      "17s - loss: 4923.9597 - custom_acc: 0.4104 - val_loss: 4869.0085 - val_custom_acc: 0.3592\n",
      "Epoch 202/500\n",
      "17s - loss: 4923.9171 - custom_acc: 0.4106 - val_loss: 4868.8838 - val_custom_acc: 0.3605\n",
      "Epoch 203/500\n",
      "17s - loss: 4923.7986 - custom_acc: 0.4113 - val_loss: 4868.7412 - val_custom_acc: 0.3621\n",
      "Epoch 204/500\n",
      "17s - loss: 4923.6721 - custom_acc: 0.4121 - val_loss: 4868.6792 - val_custom_acc: 0.3629\n",
      "Epoch 205/500\n",
      "18s - loss: 4923.5622 - custom_acc: 0.4126 - val_loss: 4868.5691 - val_custom_acc: 0.3640\n",
      "Epoch 206/500\n",
      "18s - loss: 4923.4125 - custom_acc: 0.4133 - val_loss: 4868.4854 - val_custom_acc: 0.3651\n",
      "Epoch 207/500\n",
      "17s - loss: 4923.2988 - custom_acc: 0.4140 - val_loss: 4868.4816 - val_custom_acc: 0.3655\n",
      "Epoch 208/500\n",
      "18s - loss: 4923.1324 - custom_acc: 0.4149 - val_loss: 4868.3479 - val_custom_acc: 0.3667\n",
      "Epoch 209/500\n",
      "18s - loss: 4923.0442 - custom_acc: 0.4154 - val_loss: 4868.2995 - val_custom_acc: 0.3676\n",
      "Epoch 210/500\n",
      "17s - loss: 4922.8871 - custom_acc: 0.4162 - val_loss: 4868.1849 - val_custom_acc: 0.3684\n",
      "Epoch 211/500\n",
      "17s - loss: 4922.7720 - custom_acc: 0.4170 - val_loss: 4868.1561 - val_custom_acc: 0.3689\n",
      "Epoch 212/500\n",
      "18s - loss: 4922.6446 - custom_acc: 0.4177 - val_loss: 4868.0237 - val_custom_acc: 0.3700\n",
      "Epoch 213/500\n",
      "17s - loss: 4922.4873 - custom_acc: 0.4185 - val_loss: 4867.9388 - val_custom_acc: 0.3711\n",
      "Epoch 214/500\n",
      "17s - loss: 4922.3219 - custom_acc: 0.4194 - val_loss: 4867.8766 - val_custom_acc: 0.3719\n",
      "Epoch 215/500\n",
      "17s - loss: 4922.2019 - custom_acc: 0.4201 - val_loss: 4867.8128 - val_custom_acc: 0.3726\n",
      "Epoch 216/500\n",
      "18s - loss: 4922.0555 - custom_acc: 0.4209 - val_loss: 4867.6789 - val_custom_acc: 0.3740\n",
      "Epoch 217/500\n",
      "18s - loss: 4921.8766 - custom_acc: 0.4218 - val_loss: 4867.5926 - val_custom_acc: 0.3749\n",
      "Epoch 218/500\n",
      "17s - loss: 4921.7656 - custom_acc: 0.4224 - val_loss: 4867.6181 - val_custom_acc: 0.3748\n",
      "Epoch 219/500\n",
      "17s - loss: 4921.6167 - custom_acc: 0.4231 - val_loss: 4867.4607 - val_custom_acc: 0.3767\n",
      "Epoch 220/500\n",
      "18s - loss: 4921.4668 - custom_acc: 0.4238 - val_loss: 4867.4040 - val_custom_acc: 0.3774\n",
      "Epoch 221/500\n",
      "18s - loss: 4921.3244 - custom_acc: 0.4246 - val_loss: 4867.3189 - val_custom_acc: 0.3785\n",
      "Epoch 222/500\n",
      "17s - loss: 4921.1633 - custom_acc: 0.4253 - val_loss: 4867.2441 - val_custom_acc: 0.3795\n",
      "Epoch 223/500\n",
      "17s - loss: 4921.0323 - custom_acc: 0.4259 - val_loss: 4867.1996 - val_custom_acc: 0.3800\n",
      "Epoch 224/500\n",
      "17s - loss: 4920.8717 - custom_acc: 0.4267 - val_loss: 4867.1290 - val_custom_acc: 0.3808\n",
      "Epoch 225/500\n",
      "17s - loss: 4920.7158 - custom_acc: 0.4274 - val_loss: 4867.0718 - val_custom_acc: 0.3813\n",
      "Epoch 226/500\n",
      "17s - loss: 4920.5698 - custom_acc: 0.4281 - val_loss: 4867.0158 - val_custom_acc: 0.3817\n",
      "Epoch 227/500\n",
      "17s - loss: 4920.3993 - custom_acc: 0.4288 - val_loss: 4866.9301 - val_custom_acc: 0.3826\n",
      "Epoch 228/500\n",
      "17s - loss: 4920.2570 - custom_acc: 0.4295 - val_loss: 4866.8868 - val_custom_acc: 0.3829\n",
      "Epoch 229/500\n",
      "17s - loss: 4920.1043 - custom_acc: 0.4302 - val_loss: 4866.8403 - val_custom_acc: 0.3838\n",
      "Epoch 230/500\n",
      "17s - loss: 4919.9366 - custom_acc: 0.4308 - val_loss: 4866.7800 - val_custom_acc: 0.3842\n",
      "Epoch 231/500\n",
      "17s - loss: 4919.7890 - custom_acc: 0.4315 - val_loss: 4866.7062 - val_custom_acc: 0.3852\n",
      "Epoch 232/500\n",
      "18s - loss: 4919.6230 - custom_acc: 0.4321 - val_loss: 4866.6397 - val_custom_acc: 0.3859\n",
      "Epoch 233/500\n",
      "18s - loss: 4919.4703 - custom_acc: 0.4327 - val_loss: 4866.5852 - val_custom_acc: 0.3862\n",
      "Epoch 234/500\n",
      "17s - loss: 4919.3338 - custom_acc: 0.4333 - val_loss: 4866.5542 - val_custom_acc: 0.3865\n",
      "Epoch 235/500\n",
      "18s - loss: 4919.1523 - custom_acc: 0.4340 - val_loss: 4866.4832 - val_custom_acc: 0.3871\n",
      "Epoch 236/500\n",
      "17s - loss: 4918.9957 - custom_acc: 0.4347 - val_loss: 4866.4743 - val_custom_acc: 0.3877\n",
      "Epoch 237/500\n",
      "17s - loss: 4918.8073 - custom_acc: 0.4354 - val_loss: 4866.3479 - val_custom_acc: 0.3888\n",
      "Epoch 238/500\n",
      "18s - loss: 4918.6516 - custom_acc: 0.4360 - val_loss: 4866.2826 - val_custom_acc: 0.3895\n",
      "Epoch 239/500\n",
      "17s - loss: 4918.5139 - custom_acc: 0.4366 - val_loss: 4866.2473 - val_custom_acc: 0.3900\n",
      "Epoch 240/500\n",
      "17s - loss: 4918.3292 - custom_acc: 0.4373 - val_loss: 4866.1863 - val_custom_acc: 0.3907\n",
      "Epoch 241/500\n",
      "17s - loss: 4918.1728 - custom_acc: 0.4379 - val_loss: 4866.1230 - val_custom_acc: 0.3909\n",
      "Epoch 242/500\n",
      "18s - loss: 4918.0097 - custom_acc: 0.4385 - val_loss: 4866.0537 - val_custom_acc: 0.3913\n",
      "Epoch 243/500\n",
      "17s - loss: 4917.8279 - custom_acc: 0.4392 - val_loss: 4865.9918 - val_custom_acc: 0.3917\n",
      "Epoch 244/500\n",
      "18s - loss: 4917.6620 - custom_acc: 0.4398 - val_loss: 4865.9450 - val_custom_acc: 0.3923\n",
      "Epoch 245/500\n",
      "17s - loss: 4917.4992 - custom_acc: 0.4404 - val_loss: 4865.8977 - val_custom_acc: 0.3930\n",
      "Epoch 246/500\n",
      "17s - loss: 4917.3151 - custom_acc: 0.4411 - val_loss: 4865.8212 - val_custom_acc: 0.3934\n",
      "Epoch 247/500\n",
      "17s - loss: 4917.1529 - custom_acc: 0.4418 - val_loss: 4865.8495 - val_custom_acc: 0.3931\n",
      "Epoch 248/500\n",
      "17s - loss: 4916.9959 - custom_acc: 0.4424 - val_loss: 4865.7560 - val_custom_acc: 0.3941\n",
      "Epoch 249/500\n",
      "17s - loss: 4916.7785 - custom_acc: 0.4432 - val_loss: 4865.6695 - val_custom_acc: 0.3947\n",
      "Epoch 250/500\n",
      "17s - loss: 4916.6530 - custom_acc: 0.4437 - val_loss: 4865.6070 - val_custom_acc: 0.3955\n",
      "Epoch 251/500\n",
      "17s - loss: 4916.4578 - custom_acc: 0.4444 - val_loss: 4865.5868 - val_custom_acc: 0.3956\n",
      "Epoch 252/500\n",
      "18s - loss: 4916.2743 - custom_acc: 0.4450 - val_loss: 4865.4855 - val_custom_acc: 0.3968\n",
      "Epoch 253/500\n",
      "18s - loss: 4916.1118 - custom_acc: 0.4455 - val_loss: 4865.4532 - val_custom_acc: 0.3971\n",
      "Epoch 254/500\n",
      "18s - loss: 4915.9556 - custom_acc: 0.4460 - val_loss: 4865.3993 - val_custom_acc: 0.3974\n",
      "Epoch 255/500\n",
      "17s - loss: 4915.7469 - custom_acc: 0.4467 - val_loss: 4865.3137 - val_custom_acc: 0.3978\n",
      "Epoch 256/500\n",
      "17s - loss: 4915.5518 - custom_acc: 0.4473 - val_loss: 4865.2528 - val_custom_acc: 0.3981\n",
      "Epoch 257/500\n",
      "17s - loss: 4915.4013 - custom_acc: 0.4478 - val_loss: 4865.2138 - val_custom_acc: 0.3985\n",
      "Epoch 258/500\n",
      "18s - loss: 4915.2312 - custom_acc: 0.4484 - val_loss: 4865.1714 - val_custom_acc: 0.3988\n",
      "Epoch 259/500\n",
      "18s - loss: 4915.0134 - custom_acc: 0.4491 - val_loss: 4865.1021 - val_custom_acc: 0.3992\n",
      "Epoch 260/500\n",
      "17s - loss: 4914.8745 - custom_acc: 0.4495 - val_loss: 4865.0658 - val_custom_acc: 0.3996\n",
      "Epoch 261/500\n",
      "18s - loss: 4914.6728 - custom_acc: 0.4501 - val_loss: 4865.0272 - val_custom_acc: 0.3998\n",
      "Epoch 262/500\n",
      "18s - loss: 4914.4589 - custom_acc: 0.4508 - val_loss: 4864.9347 - val_custom_acc: 0.4009\n",
      "Epoch 263/500\n",
      "17s - loss: 4914.2835 - custom_acc: 0.4513 - val_loss: 4864.8974 - val_custom_acc: 0.4011\n",
      "Epoch 264/500\n",
      "18s - loss: 4914.1316 - custom_acc: 0.4518 - val_loss: 4864.8390 - val_custom_acc: 0.4017\n",
      "Epoch 265/500\n",
      "18s - loss: 4913.9299 - custom_acc: 0.4524 - val_loss: 4864.8439 - val_custom_acc: 0.4011\n",
      "Epoch 266/500\n",
      "17s - loss: 4913.7565 - custom_acc: 0.4529 - val_loss: 4864.7808 - val_custom_acc: 0.4019\n",
      "Epoch 267/500\n",
      "17s - loss: 4913.6014 - custom_acc: 0.4534 - val_loss: 4864.7227 - val_custom_acc: 0.4025\n",
      "Epoch 268/500\n",
      "18s - loss: 4913.3610 - custom_acc: 0.4541 - val_loss: 4864.6459 - val_custom_acc: 0.4032\n",
      "Epoch 269/500\n",
      "18s - loss: 4913.2254 - custom_acc: 0.4545 - val_loss: 4864.6195 - val_custom_acc: 0.4030\n",
      "Epoch 270/500\n",
      "17s - loss: 4912.9702 - custom_acc: 0.4553 - val_loss: 4864.5721 - val_custom_acc: 0.4034\n",
      "Epoch 271/500\n",
      "17s - loss: 4912.8198 - custom_acc: 0.4558 - val_loss: 4864.5075 - val_custom_acc: 0.4043\n",
      "Epoch 272/500\n",
      "18s - loss: 4912.6309 - custom_acc: 0.4563 - val_loss: 4864.4645 - val_custom_acc: 0.4049\n",
      "Epoch 273/500\n",
      "18s - loss: 4912.4198 - custom_acc: 0.4570 - val_loss: 4864.4225 - val_custom_acc: 0.4046\n",
      "Epoch 274/500\n",
      "17s - loss: 4912.2739 - custom_acc: 0.4574 - val_loss: 4864.3803 - val_custom_acc: 0.4055\n",
      "Epoch 275/500\n",
      "17s - loss: 4912.0681 - custom_acc: 0.4579 - val_loss: 4864.3343 - val_custom_acc: 0.4058\n",
      "Epoch 276/500\n",
      "18s - loss: 4911.9186 - custom_acc: 0.4584 - val_loss: 4864.2927 - val_custom_acc: 0.4059\n",
      "Epoch 277/500\n",
      "17s - loss: 4911.6612 - custom_acc: 0.4590 - val_loss: 4864.2745 - val_custom_acc: 0.4068\n",
      "Epoch 278/500\n",
      "17s - loss: 4911.5287 - custom_acc: 0.4595 - val_loss: 4864.2179 - val_custom_acc: 0.4069\n",
      "Epoch 279/500\n",
      "17s - loss: 4911.3229 - custom_acc: 0.4600 - val_loss: 4864.1707 - val_custom_acc: 0.4070\n",
      "Epoch 280/500\n",
      "18s - loss: 4911.1412 - custom_acc: 0.4605 - val_loss: 4864.1449 - val_custom_acc: 0.4073\n",
      "Epoch 281/500\n",
      "18s - loss: 4910.9487 - custom_acc: 0.4610 - val_loss: 4864.0994 - val_custom_acc: 0.4075\n",
      "Epoch 282/500\n",
      "17s - loss: 4910.8059 - custom_acc: 0.4614 - val_loss: 4864.0483 - val_custom_acc: 0.4076\n",
      "Epoch 283/500\n",
      "17s - loss: 4910.5100 - custom_acc: 0.4621 - val_loss: 4864.0085 - val_custom_acc: 0.4079\n",
      "Epoch 284/500\n",
      "18s - loss: 4910.3885 - custom_acc: 0.4625 - val_loss: 4863.9824 - val_custom_acc: 0.4081\n",
      "Epoch 285/500\n",
      "18s - loss: 4910.2083 - custom_acc: 0.4630 - val_loss: 4863.9412 - val_custom_acc: 0.4084\n",
      "Epoch 286/500\n",
      "17s - loss: 4909.9768 - custom_acc: 0.4635 - val_loss: 4863.8970 - val_custom_acc: 0.4084\n",
      "Epoch 287/500\n",
      "17s - loss: 4909.9211 - custom_acc: 0.4637 - val_loss: 4863.8648 - val_custom_acc: 0.4085\n",
      "Epoch 288/500\n",
      "18s - loss: 4909.5770 - custom_acc: 0.4645 - val_loss: 4863.8257 - val_custom_acc: 0.4089\n",
      "Epoch 289/500\n",
      "18s - loss: 4909.4413 - custom_acc: 0.4650 - val_loss: 4863.7709 - val_custom_acc: 0.4084\n",
      "Epoch 290/500\n",
      "17s - loss: 4909.2505 - custom_acc: 0.4651 - val_loss: 4863.7882 - val_custom_acc: 0.4078\n",
      "Epoch 291/500\n",
      "18s - loss: 4909.0879 - custom_acc: 0.4656 - val_loss: 4863.7742 - val_custom_acc: 0.4094\n",
      "Epoch 292/500\n",
      "18s - loss: 4908.9713 - custom_acc: 0.4659 - val_loss: 4863.6943 - val_custom_acc: 0.4093\n",
      "Epoch 293/500\n",
      "18s - loss: 4908.6570 - custom_acc: 0.4666 - val_loss: 4863.6595 - val_custom_acc: 0.4099\n",
      "Epoch 294/500\n",
      "18s - loss: 4908.5170 - custom_acc: 0.4670 - val_loss: 4863.6793 - val_custom_acc: 0.4101\n",
      "Epoch 295/500\n",
      "17s - loss: 4908.2686 - custom_acc: 0.4676 - val_loss: 4863.6091 - val_custom_acc: 0.4090\n",
      "Epoch 296/500\n",
      "18s - loss: 4908.2058 - custom_acc: 0.4678 - val_loss: 4863.6101 - val_custom_acc: 0.4104\n",
      "Epoch 297/500\n",
      "18s - loss: 4907.9478 - custom_acc: 0.4682 - val_loss: 4863.5095 - val_custom_acc: 0.4103\n",
      "Epoch 298/500\n",
      "17s - loss: 4907.7449 - custom_acc: 0.4687 - val_loss: 4863.5454 - val_custom_acc: 0.4094\n",
      "Epoch 299/500\n",
      "17s - loss: 4907.5576 - custom_acc: 0.4690 - val_loss: 4863.7787 - val_custom_acc: 0.4078\n",
      "Epoch 300/500\n",
      "18s - loss: 4907.3840 - custom_acc: 0.4693 - val_loss: 4863.4205 - val_custom_acc: 0.4104\n",
      "Epoch 301/500\n",
      "18s - loss: 4907.3124 - custom_acc: 0.4696 - val_loss: 4863.9209 - val_custom_acc: 0.4070\n",
      "Epoch 302/500\n",
      "17s - loss: 4906.9817 - custom_acc: 0.4703 - val_loss: 4863.3896 - val_custom_acc: 0.4110\n",
      "Epoch 303/500\n",
      "18s - loss: 4906.9282 - custom_acc: 0.4707 - val_loss: 4863.3765 - val_custom_acc: 0.4111\n",
      "Epoch 304/500\n",
      "18s - loss: 4906.7022 - custom_acc: 0.4712 - val_loss: 4863.5018 - val_custom_acc: 0.4117\n",
      "Epoch 305/500\n",
      "18s - loss: 4906.4814 - custom_acc: 0.4717 - val_loss: 4863.2971 - val_custom_acc: 0.4109\n",
      "Epoch 306/500\n",
      "17s - loss: 4906.4817 - custom_acc: 0.4718 - val_loss: 4863.3237 - val_custom_acc: 0.4104\n",
      "Epoch 307/500\n",
      "17s - loss: 4906.0004 - custom_acc: 0.4726 - val_loss: 4863.2624 - val_custom_acc: 0.4112\n",
      "Epoch 308/500\n",
      "17s - loss: 4905.9489 - custom_acc: 0.4729 - val_loss: 4863.4157 - val_custom_acc: 0.4100\n",
      "Epoch 309/500\n",
      "17s - loss: 4905.9097 - custom_acc: 0.4725 - val_loss: 4863.3447 - val_custom_acc: 0.4116\n",
      "Epoch 310/500\n",
      "18s - loss: 4905.6387 - custom_acc: 0.4734 - val_loss: 4863.6053 - val_custom_acc: 0.4092\n",
      "Epoch 311/500\n",
      "18s - loss: 4905.8033 - custom_acc: 0.4725 - val_loss: 4863.5413 - val_custom_acc: 0.4098\n",
      "Epoch 312/500\n",
      "17s - loss: 4905.4064 - custom_acc: 0.4736 - val_loss: 4863.4289 - val_custom_acc: 0.4104\n",
      "Epoch 313/500\n",
      "17s - loss: 4904.9431 - custom_acc: 0.4749 - val_loss: 4863.2055 - val_custom_acc: 0.4124\n",
      "Epoch 314/500\n",
      "18s - loss: 4905.0434 - custom_acc: 0.4747 - val_loss: 4863.2497 - val_custom_acc: 0.4107\n",
      "Epoch 315/500\n",
      "17s - loss: 4904.9446 - custom_acc: 0.4746 - val_loss: 4863.3640 - val_custom_acc: 0.4104\n",
      "Epoch 316/500\n",
      "17s - loss: 4904.6714 - custom_acc: 0.4753 - val_loss: 4863.2405 - val_custom_acc: 0.4108\n",
      "Epoch 317/500\n",
      "17s - loss: 4904.2889 - custom_acc: 0.4765 - val_loss: 4863.1288 - val_custom_acc: 0.4122\n",
      "Epoch 318/500\n",
      "17s - loss: 4904.3385 - custom_acc: 0.4760 - val_loss: 4863.2948 - val_custom_acc: 0.4108\n",
      "Epoch 319/500\n",
      "18s - loss: 4904.3455 - custom_acc: 0.4758 - val_loss: 4863.5241 - val_custom_acc: 0.4099\n",
      "Epoch 320/500\n",
      "18s - loss: 4903.5432 - custom_acc: 0.4779 - val_loss: 4863.0111 - val_custom_acc: 0.4118\n",
      "Epoch 321/500\n",
      "17s - loss: 4903.8833 - custom_acc: 0.4772 - val_loss: 4864.2936 - val_custom_acc: 0.4059\n",
      "Epoch 322/500\n",
      "18s - loss: 4903.8898 - custom_acc: 0.4767 - val_loss: 4863.3615 - val_custom_acc: 0.4110\n",
      "Epoch 323/500\n",
      "17s - loss: 4903.3495 - custom_acc: 0.4784 - val_loss: 4863.0268 - val_custom_acc: 0.4123\n",
      "Epoch 324/500\n",
      "17s - loss: 4903.2062 - custom_acc: 0.4787 - val_loss: 4863.0224 - val_custom_acc: 0.4127\n",
      "Epoch 325/500\n",
      "18s - loss: 4903.2981 - custom_acc: 0.4783 - val_loss: 4863.4356 - val_custom_acc: 0.4107\n",
      "Epoch 326/500\n",
      "17s - loss: 4902.7711 - custom_acc: 0.4793 - val_loss: 4863.3116 - val_custom_acc: 0.4111\n",
      "Epoch 327/500\n",
      "18s - loss: 4902.8600 - custom_acc: 0.4790 - val_loss: 4863.1037 - val_custom_acc: 0.4121\n",
      "Epoch 328/500\n",
      "17s - loss: 4902.6516 - custom_acc: 0.4798 - val_loss: 4863.1124 - val_custom_acc: 0.4121\n",
      "Epoch 329/500\n",
      "17s - loss: 4902.4902 - custom_acc: 0.4801 - val_loss: 4863.1109 - val_custom_acc: 0.4122\n",
      "Epoch 330/500\n",
      "18s - loss: 4902.1853 - custom_acc: 0.4808 - val_loss: 4862.9898 - val_custom_acc: 0.4131\n",
      "Epoch 331/500\n",
      "18s - loss: 4901.8806 - custom_acc: 0.4815 - val_loss: 4863.5741 - val_custom_acc: 0.4097\n",
      "Epoch 332/500\n",
      "18s - loss: 4902.0319 - custom_acc: 0.4810 - val_loss: 4862.9598 - val_custom_acc: 0.4133\n",
      "Epoch 333/500\n",
      "18s - loss: 4901.6867 - custom_acc: 0.4820 - val_loss: 4863.0456 - val_custom_acc: 0.4124\n",
      "Epoch 334/500\n",
      "17s - loss: 4901.9707 - custom_acc: 0.4813 - val_loss: 4863.3157 - val_custom_acc: 0.4113\n",
      "Epoch 335/500\n",
      "17s - loss: 4901.3622 - custom_acc: 0.4822 - val_loss: 4863.5156 - val_custom_acc: 0.4101\n",
      "Epoch 336/500\n",
      "18s - loss: 4901.4429 - custom_acc: 0.4820 - val_loss: 4863.2419 - val_custom_acc: 0.4116\n",
      "Epoch 337/500\n",
      "17s - loss: 4901.1021 - custom_acc: 0.4828 - val_loss: 4863.2341 - val_custom_acc: 0.4118\n",
      "Epoch 338/500\n",
      "17s - loss: 4900.8495 - custom_acc: 0.4835 - val_loss: 4862.9890 - val_custom_acc: 0.4131\n",
      "Epoch 339/500\n",
      "18s - loss: 4900.7948 - custom_acc: 0.4837 - val_loss: 4864.9754 - val_custom_acc: 0.4041\n",
      "Epoch 340/500\n",
      "18s - loss: 4900.6699 - custom_acc: 0.4836 - val_loss: 4864.7161 - val_custom_acc: 0.4056\n",
      "Epoch 341/500\n",
      "18s - loss: 4900.4813 - custom_acc: 0.4838 - val_loss: 4863.9143 - val_custom_acc: 0.4089\n",
      "Epoch 342/500\n",
      "18s - loss: 4900.5430 - custom_acc: 0.4836 - val_loss: 4863.3107 - val_custom_acc: 0.4112\n",
      "Epoch 343/500\n",
      "18s - loss: 4900.2041 - custom_acc: 0.4848 - val_loss: 4863.0490 - val_custom_acc: 0.4129\n",
      "Epoch 344/500\n",
      "18s - loss: 4899.8318 - custom_acc: 0.4857 - val_loss: 4864.4639 - val_custom_acc: 0.4067\n",
      "Epoch 345/500\n",
      "18s - loss: 4900.2189 - custom_acc: 0.4840 - val_loss: 4863.4694 - val_custom_acc: 0.4110\n",
      "Epoch 346/500\n",
      "18s - loss: 4899.4935 - custom_acc: 0.4860 - val_loss: 4863.7857 - val_custom_acc: 0.4094\n",
      "Epoch 347/500\n",
      "18s - loss: 4899.6596 - custom_acc: 0.4859 - val_loss: 4863.0198 - val_custom_acc: 0.4134\n",
      "Epoch 348/500\n",
      "18s - loss: 4899.4044 - custom_acc: 0.4865 - val_loss: 4864.2583 - val_custom_acc: 0.4079\n",
      "Epoch 349/500\n",
      "18s - loss: 4899.4010 - custom_acc: 0.4864 - val_loss: 4863.0469 - val_custom_acc: 0.4135\n",
      "Epoch 350/500\n",
      "18s - loss: 4898.9864 - custom_acc: 0.4873 - val_loss: 4864.2412 - val_custom_acc: 0.4081\n",
      "Epoch 351/500\n",
      "18s - loss: 4899.2373 - custom_acc: 0.4865 - val_loss: 4863.1112 - val_custom_acc: 0.4134\n",
      "Epoch 352/500\n",
      "18s - loss: 4898.5139 - custom_acc: 0.4884 - val_loss: 4863.1713 - val_custom_acc: 0.4128\n",
      "Epoch 353/500\n",
      "18s - loss: 4898.6558 - custom_acc: 0.4880 - val_loss: 4864.0746 - val_custom_acc: 0.4090\n",
      "Epoch 354/500\n",
      "18s - loss: 4898.8196 - custom_acc: 0.4875 - val_loss: 4863.1748 - val_custom_acc: 0.4137\n",
      "Epoch 355/500\n",
      "18s - loss: 4898.2900 - custom_acc: 0.4882 - val_loss: 4863.3538 - val_custom_acc: 0.4122\n",
      "Epoch 356/500\n",
      "18s - loss: 4898.3733 - custom_acc: 0.4884 - val_loss: 4863.2201 - val_custom_acc: 0.4134\n",
      "Epoch 357/500\n",
      "18s - loss: 4897.9664 - custom_acc: 0.4894 - val_loss: 4863.1096 - val_custom_acc: 0.4144\n",
      "Epoch 358/500\n",
      "18s - loss: 4897.7282 - custom_acc: 0.4899 - val_loss: 4863.0605 - val_custom_acc: 0.4147\n",
      "Epoch 359/500\n",
      "18s - loss: 4897.6543 - custom_acc: 0.4901 - val_loss: 4863.9723 - val_custom_acc: 0.4097\n",
      "Epoch 360/500\n",
      "18s - loss: 4897.9998 - custom_acc: 0.4892 - val_loss: 4863.2693 - val_custom_acc: 0.4136\n",
      "Epoch 361/500\n",
      "18s - loss: 4897.4998 - custom_acc: 0.4903 - val_loss: 4863.2561 - val_custom_acc: 0.4139\n",
      "Epoch 362/500\n",
      "18s - loss: 4897.2763 - custom_acc: 0.4909 - val_loss: 4863.2242 - val_custom_acc: 0.4142\n",
      "Epoch 363/500\n",
      "18s - loss: 4896.9669 - custom_acc: 0.4915 - val_loss: 4863.1411 - val_custom_acc: 0.4145\n",
      "Epoch 364/500\n",
      "18s - loss: 4897.0140 - custom_acc: 0.4914 - val_loss: 4864.7913 - val_custom_acc: 0.4063\n",
      "Epoch 365/500\n",
      "18s - loss: 4897.1640 - custom_acc: 0.4909 - val_loss: 4863.3191 - val_custom_acc: 0.4140\n",
      "Epoch 366/500\n",
      "18s - loss: 4896.7033 - custom_acc: 0.4919 - val_loss: 4863.3503 - val_custom_acc: 0.4135\n",
      "Epoch 367/500\n",
      "18s - loss: 4896.6205 - custom_acc: 0.4921 - val_loss: 4863.3431 - val_custom_acc: 0.4137\n",
      "Epoch 368/500\n",
      "18s - loss: 4896.3630 - custom_acc: 0.4927 - val_loss: 4863.3721 - val_custom_acc: 0.4135\n",
      "Epoch 369/500\n",
      "18s - loss: 4896.3107 - custom_acc: 0.4928 - val_loss: 4863.3777 - val_custom_acc: 0.4137\n",
      "Epoch 370/500\n",
      "18s - loss: 4896.0054 - custom_acc: 0.4931 - val_loss: 4863.5187 - val_custom_acc: 0.4125\n",
      "Epoch 371/500\n",
      "18s - loss: 4895.9938 - custom_acc: 0.4936 - val_loss: 4863.3174 - val_custom_acc: 0.4142\n",
      "Epoch 372/500\n",
      "18s - loss: 4895.9035 - custom_acc: 0.4937 - val_loss: 4863.3244 - val_custom_acc: 0.4144\n",
      "Epoch 373/500\n",
      "18s - loss: 4895.6749 - custom_acc: 0.4941 - val_loss: 4863.3294 - val_custom_acc: 0.4144\n",
      "Epoch 374/500\n",
      "18s - loss: 4895.6044 - custom_acc: 0.4943 - val_loss: 4863.3688 - val_custom_acc: 0.4144\n",
      "Epoch 375/500\n",
      "18s - loss: 4895.4345 - custom_acc: 0.4946 - val_loss: 4863.3933 - val_custom_acc: 0.4143\n",
      "Epoch 376/500\n",
      "18s - loss: 4895.1475 - custom_acc: 0.4953 - val_loss: 4863.3830 - val_custom_acc: 0.4143\n",
      "Epoch 377/500\n",
      "18s - loss: 4895.0081 - custom_acc: 0.4957 - val_loss: 4864.2328 - val_custom_acc: 0.4100\n",
      "Epoch 378/500\n",
      "17s - loss: 4895.4131 - custom_acc: 0.4945 - val_loss: 4863.6319 - val_custom_acc: 0.4130\n",
      "Epoch 379/500\n",
      "17s - loss: 4894.9773 - custom_acc: 0.4956 - val_loss: 4863.6712 - val_custom_acc: 0.4128\n",
      "Epoch 380/500\n",
      "18s - loss: 4894.7781 - custom_acc: 0.4960 - val_loss: 4863.7403 - val_custom_acc: 0.4124\n",
      "Epoch 381/500\n",
      "18s - loss: 4894.6436 - custom_acc: 0.4963 - val_loss: 4863.6709 - val_custom_acc: 0.4132\n",
      "Epoch 382/500\n",
      "18s - loss: 4894.4580 - custom_acc: 0.4966 - val_loss: 4863.6960 - val_custom_acc: 0.4131\n",
      "Epoch 383/500\n",
      "18s - loss: 4894.2946 - custom_acc: 0.4970 - val_loss: 4863.8097 - val_custom_acc: 0.4125\n",
      "Epoch 384/500\n",
      "18s - loss: 4894.1179 - custom_acc: 0.4974 - val_loss: 4863.8471 - val_custom_acc: 0.4124\n",
      "Epoch 385/500\n",
      "17s - loss: 4894.0156 - custom_acc: 0.4976 - val_loss: 4863.7613 - val_custom_acc: 0.4131\n",
      "Epoch 386/500\n",
      "17s - loss: 4893.8205 - custom_acc: 0.4980 - val_loss: 4863.7824 - val_custom_acc: 0.4132\n",
      "Epoch 387/500\n",
      "18s - loss: 4893.6485 - custom_acc: 0.4983 - val_loss: 4863.8795 - val_custom_acc: 0.4126\n",
      "Epoch 388/500\n",
      "18s - loss: 4893.5241 - custom_acc: 0.4986 - val_loss: 4863.9486 - val_custom_acc: 0.4124\n",
      "Epoch 389/500\n",
      "18s - loss: 4893.4331 - custom_acc: 0.4988 - val_loss: 4863.8505 - val_custom_acc: 0.4131\n",
      "Epoch 390/500\n",
      "18s - loss: 4893.4571 - custom_acc: 0.4983 - val_loss: 4863.8072 - val_custom_acc: 0.4135\n",
      "Epoch 391/500\n",
      "17s - loss: 4893.2970 - custom_acc: 0.4988 - val_loss: 4863.8218 - val_custom_acc: 0.4134\n",
      "Epoch 392/500\n",
      "17s - loss: 4893.0039 - custom_acc: 0.4995 - val_loss: 4863.8301 - val_custom_acc: 0.4135\n",
      "Epoch 393/500\n",
      "18s - loss: 4892.9077 - custom_acc: 0.4996 - val_loss: 4863.8253 - val_custom_acc: 0.4136\n",
      "Epoch 394/500\n",
      "18s - loss: 4892.8667 - custom_acc: 0.4999 - val_loss: 4863.8627 - val_custom_acc: 0.4138\n",
      "Epoch 395/500\n",
      "18s - loss: 4892.6152 - custom_acc: 0.5003 - val_loss: 4863.8656 - val_custom_acc: 0.4142\n",
      "Epoch 396/500\n",
      "18s - loss: 4892.5418 - custom_acc: 0.5005 - val_loss: 4865.3357 - val_custom_acc: 0.4079\n",
      "Epoch 397/500\n",
      "17s - loss: 4892.7640 - custom_acc: 0.4997 - val_loss: 4864.0374 - val_custom_acc: 0.4134\n",
      "Epoch 398/500\n",
      "18s - loss: 4892.2453 - custom_acc: 0.5012 - val_loss: 4864.0422 - val_custom_acc: 0.4133\n",
      "Epoch 399/500\n",
      "17s - loss: 4892.1517 - custom_acc: 0.5014 - val_loss: 4864.1107 - val_custom_acc: 0.4133\n",
      "Epoch 400/500\n",
      "18s - loss: 4892.0005 - custom_acc: 0.5017 - val_loss: 4864.1575 - val_custom_acc: 0.4132\n",
      "Epoch 401/500\n",
      "18s - loss: 4891.8327 - custom_acc: 0.5020 - val_loss: 4864.1906 - val_custom_acc: 0.4132\n",
      "Epoch 402/500\n",
      "18s - loss: 4891.7205 - custom_acc: 0.5023 - val_loss: 4864.1706 - val_custom_acc: 0.4133\n",
      "Epoch 403/500\n",
      "18s - loss: 4891.5426 - custom_acc: 0.5024 - val_loss: 4864.1441 - val_custom_acc: 0.4137\n",
      "Epoch 404/500\n",
      "18s - loss: 4891.4180 - custom_acc: 0.5026 - val_loss: 4864.1233 - val_custom_acc: 0.4140\n",
      "Epoch 405/500\n",
      "18s - loss: 4891.3868 - custom_acc: 0.5029 - val_loss: 4864.2158 - val_custom_acc: 0.4135\n",
      "Epoch 406/500\n",
      "18s - loss: 4891.4293 - custom_acc: 0.5027 - val_loss: 4864.2498 - val_custom_acc: 0.4136\n",
      "Epoch 407/500\n",
      "18s - loss: 4890.9408 - custom_acc: 0.5037 - val_loss: 4864.2284 - val_custom_acc: 0.4139\n",
      "Epoch 408/500\n",
      "17s - loss: 4890.7718 - custom_acc: 0.5039 - val_loss: 4864.7171 - val_custom_acc: 0.4116\n",
      "Epoch 409/500\n",
      "18s - loss: 4891.1507 - custom_acc: 0.5031 - val_loss: 4864.3544 - val_custom_acc: 0.4138\n",
      "Epoch 410/500\n",
      "17s - loss: 4890.6500 - custom_acc: 0.5040 - val_loss: 4864.3478 - val_custom_acc: 0.4138\n",
      "Epoch 411/500\n",
      "17s - loss: 4890.4381 - custom_acc: 0.5047 - val_loss: 4864.3937 - val_custom_acc: 0.4134\n",
      "Epoch 412/500\n",
      "17s - loss: 4890.7378 - custom_acc: 0.5041 - val_loss: 4864.4633 - val_custom_acc: 0.4134\n",
      "Epoch 413/500\n",
      "18s - loss: 4890.3178 - custom_acc: 0.5051 - val_loss: 4864.4823 - val_custom_acc: 0.4133\n",
      "Epoch 414/500\n",
      "18s - loss: 4890.1324 - custom_acc: 0.5054 - val_loss: 4864.5236 - val_custom_acc: 0.4133\n",
      "Epoch 415/500\n",
      "18s - loss: 4890.0240 - custom_acc: 0.5057 - val_loss: 4864.5730 - val_custom_acc: 0.4132\n",
      "Epoch 416/500\n",
      "18s - loss: 4889.8862 - custom_acc: 0.5060 - val_loss: 4864.6288 - val_custom_acc: 0.4131\n",
      "Epoch 417/500\n",
      "18s - loss: 4889.7535 - custom_acc: 0.5062 - val_loss: 4864.6063 - val_custom_acc: 0.4134\n",
      "Epoch 418/500\n",
      "17s - loss: 4889.7584 - custom_acc: 0.5057 - val_loss: 4864.6062 - val_custom_acc: 0.4133\n",
      "Epoch 419/500\n",
      "17s - loss: 4889.6978 - custom_acc: 0.5059 - val_loss: 4864.6544 - val_custom_acc: 0.4132\n",
      "Epoch 420/500\n",
      "18s - loss: 4889.4405 - custom_acc: 0.5063 - val_loss: 4864.6707 - val_custom_acc: 0.4133\n",
      "Epoch 421/500\n",
      "18s - loss: 4889.3507 - custom_acc: 0.5067 - val_loss: 4864.6888 - val_custom_acc: 0.4135\n",
      "Epoch 422/500\n",
      "17s - loss: 4889.1963 - custom_acc: 0.5070 - val_loss: 4864.7142 - val_custom_acc: 0.4130\n",
      "Epoch 423/500\n",
      "18s - loss: 4888.9503 - custom_acc: 0.5074 - val_loss: 4864.7544 - val_custom_acc: 0.4135\n",
      "Epoch 424/500\n",
      "17s - loss: 4888.9826 - custom_acc: 0.5075 - val_loss: 4866.5917 - val_custom_acc: 0.4065\n",
      "Epoch 425/500\n",
      "18s - loss: 4888.9228 - custom_acc: 0.5073 - val_loss: 4864.8363 - val_custom_acc: 0.4132\n",
      "Epoch 426/500\n",
      "17s - loss: 4888.5636 - custom_acc: 0.5084 - val_loss: 4866.4921 - val_custom_acc: 0.4070\n",
      "Epoch 427/500\n",
      "18s - loss: 4888.7789 - custom_acc: 0.5077 - val_loss: 4864.9344 - val_custom_acc: 0.4130\n",
      "Epoch 428/500\n",
      "18s - loss: 4888.3565 - custom_acc: 0.5087 - val_loss: 4864.9374 - val_custom_acc: 0.4129\n",
      "Epoch 429/500\n",
      "18s - loss: 4888.1723 - custom_acc: 0.5091 - val_loss: 4865.9218 - val_custom_acc: 0.4095\n",
      "Epoch 430/500\n",
      "17s - loss: 4888.7397 - custom_acc: 0.5078 - val_loss: 4865.0792 - val_custom_acc: 0.4128\n",
      "Epoch 431/500\n",
      "17s - loss: 4888.1508 - custom_acc: 0.5093 - val_loss: 4865.0734 - val_custom_acc: 0.4128\n",
      "Epoch 432/500\n",
      "17s - loss: 4887.9342 - custom_acc: 0.5097 - val_loss: 4865.0801 - val_custom_acc: 0.4128\n",
      "Epoch 433/500\n",
      "17s - loss: 4887.6906 - custom_acc: 0.5098 - val_loss: 4865.0834 - val_custom_acc: 0.4128\n",
      "Epoch 434/500\n",
      "17s - loss: 4887.6686 - custom_acc: 0.5101 - val_loss: 4865.1148 - val_custom_acc: 0.4129\n",
      "Epoch 435/500\n",
      "18s - loss: 4887.5958 - custom_acc: 0.5102 - val_loss: 4866.8482 - val_custom_acc: 0.4070\n",
      "Epoch 436/500\n",
      "18s - loss: 4887.8737 - custom_acc: 0.5094 - val_loss: 4865.2800 - val_custom_acc: 0.4127\n",
      "Epoch 437/500\n",
      "18s - loss: 4887.3537 - custom_acc: 0.5108 - val_loss: 4865.2734 - val_custom_acc: 0.4126\n",
      "Epoch 438/500\n",
      "18s - loss: 4887.0338 - custom_acc: 0.5111 - val_loss: 4865.2582 - val_custom_acc: 0.4127\n",
      "Epoch 439/500\n",
      "18s - loss: 4887.1042 - custom_acc: 0.5111 - val_loss: 4865.4058 - val_custom_acc: 0.4127\n",
      "Epoch 440/500\n",
      "18s - loss: 4886.9741 - custom_acc: 0.5113 - val_loss: 4865.4562 - val_custom_acc: 0.4128\n",
      "Epoch 441/500\n",
      "18s - loss: 4886.8953 - custom_acc: 0.5114 - val_loss: 4865.4496 - val_custom_acc: 0.4126\n",
      "Epoch 442/500\n",
      "17s - loss: 4886.7174 - custom_acc: 0.5118 - val_loss: 4865.4932 - val_custom_acc: 0.4126\n",
      "Epoch 443/500\n",
      "18s - loss: 4886.9969 - custom_acc: 0.5115 - val_loss: 4865.5440 - val_custom_acc: 0.4124\n",
      "Epoch 444/500\n",
      "18s - loss: 4886.6847 - custom_acc: 0.5122 - val_loss: 4865.5744 - val_custom_acc: 0.4122\n",
      "Epoch 445/500\n",
      "18s - loss: 4886.5206 - custom_acc: 0.5125 - val_loss: 4865.5822 - val_custom_acc: 0.4122\n",
      "Epoch 446/500\n",
      "18s - loss: 4886.3204 - custom_acc: 0.5130 - val_loss: 4865.6430 - val_custom_acc: 0.4122\n",
      "Epoch 447/500\n",
      "18s - loss: 4886.2201 - custom_acc: 0.5131 - val_loss: 4865.7231 - val_custom_acc: 0.4121\n",
      "Epoch 448/500\n",
      "17s - loss: 4886.0761 - custom_acc: 0.5134 - val_loss: 4865.6967 - val_custom_acc: 0.4120\n",
      "Epoch 449/500\n",
      "18s - loss: 4885.8698 - custom_acc: 0.5140 - val_loss: 4865.7649 - val_custom_acc: 0.4119\n",
      "Epoch 450/500\n",
      "17s - loss: 4885.8063 - custom_acc: 0.5140 - val_loss: 4865.8602 - val_custom_acc: 0.4119\n",
      "Epoch 451/500\n",
      "18s - loss: 4885.6651 - custom_acc: 0.5143 - val_loss: 4865.8440 - val_custom_acc: 0.4119\n",
      "Epoch 452/500\n",
      "17s - loss: 4885.5207 - custom_acc: 0.5146 - val_loss: 4865.9637 - val_custom_acc: 0.4118\n",
      "Epoch 453/500\n",
      "18s - loss: 4885.5129 - custom_acc: 0.5143 - val_loss: 4865.9715 - val_custom_acc: 0.4119\n",
      "Epoch 454/500\n",
      "17s - loss: 4885.4877 - custom_acc: 0.5144 - val_loss: 4865.9439 - val_custom_acc: 0.4118\n",
      "Epoch 455/500\n",
      "17s - loss: 4885.3618 - custom_acc: 0.5146 - val_loss: 4866.0088 - val_custom_acc: 0.4118\n",
      "Epoch 456/500\n",
      "18s - loss: 4885.1895 - custom_acc: 0.5150 - val_loss: 4866.1006 - val_custom_acc: 0.4117\n",
      "Epoch 457/500\n",
      "18s - loss: 4885.1164 - custom_acc: 0.5150 - val_loss: 4866.0673 - val_custom_acc: 0.4121\n",
      "Epoch 458/500\n",
      "17s - loss: 4885.0026 - custom_acc: 0.5158 - val_loss: 4866.0782 - val_custom_acc: 0.4119\n",
      "Epoch 459/500\n",
      "17s - loss: 4884.7975 - custom_acc: 0.5160 - val_loss: 4866.1312 - val_custom_acc: 0.4117\n",
      "Epoch 460/500\n",
      "18s - loss: 4884.6860 - custom_acc: 0.5160 - val_loss: 4866.1904 - val_custom_acc: 0.4115\n",
      "Epoch 461/500\n",
      "17s - loss: 4884.5304 - custom_acc: 0.5163 - val_loss: 4866.1909 - val_custom_acc: 0.4117\n",
      "Epoch 462/500\n",
      "17s - loss: 4884.4072 - custom_acc: 0.5168 - val_loss: 4866.3030 - val_custom_acc: 0.4113\n",
      "Epoch 463/500\n",
      "18s - loss: 4884.2625 - custom_acc: 0.5169 - val_loss: 4866.2961 - val_custom_acc: 0.4114\n",
      "Epoch 464/500\n",
      "18s - loss: 4884.1566 - custom_acc: 0.5173 - val_loss: 4866.5913 - val_custom_acc: 0.4106\n",
      "Epoch 465/500\n",
      "17s - loss: 4884.4701 - custom_acc: 0.5159 - val_loss: 4866.4992 - val_custom_acc: 0.4111\n",
      "Epoch 466/500\n",
      "18s - loss: 4884.1421 - custom_acc: 0.5169 - val_loss: 4866.4959 - val_custom_acc: 0.4113\n",
      "Epoch 467/500\n",
      "18s - loss: 4884.0731 - custom_acc: 0.5171 - val_loss: 4866.5474 - val_custom_acc: 0.4110\n",
      "Epoch 468/500\n",
      "19s - loss: 4883.7914 - custom_acc: 0.5176 - val_loss: 4866.5835 - val_custom_acc: 0.4111\n",
      "Epoch 469/500\n",
      "19s - loss: 4883.7550 - custom_acc: 0.5177 - val_loss: 4866.5994 - val_custom_acc: 0.4109\n",
      "Epoch 470/500\n",
      "20s - loss: 4883.4514 - custom_acc: 0.5183 - val_loss: 4866.6650 - val_custom_acc: 0.4111\n",
      "Epoch 471/500\n",
      "20s - loss: 4883.5585 - custom_acc: 0.5181 - val_loss: 4866.7228 - val_custom_acc: 0.4108\n",
      "Epoch 472/500\n",
      "20s - loss: 4883.2412 - custom_acc: 0.5188 - val_loss: 4866.7539 - val_custom_acc: 0.4111\n",
      "Epoch 473/500\n",
      "18s - loss: 4883.2596 - custom_acc: 0.5188 - val_loss: 4866.7469 - val_custom_acc: 0.4112\n",
      "Epoch 474/500\n",
      "18s - loss: 4883.2235 - custom_acc: 0.5191 - val_loss: 4866.7457 - val_custom_acc: 0.4114\n",
      "Epoch 475/500\n",
      "18s - loss: 4883.1347 - custom_acc: 0.5192 - val_loss: 4866.7639 - val_custom_acc: 0.4115\n",
      "Epoch 476/500\n",
      "18s - loss: 4882.9478 - custom_acc: 0.5197 - val_loss: 4866.8492 - val_custom_acc: 0.4107\n",
      "Epoch 477/500\n",
      "18s - loss: 4882.8080 - custom_acc: 0.5197 - val_loss: 4866.9426 - val_custom_acc: 0.4105\n",
      "Epoch 478/500\n",
      "18s - loss: 4882.6707 - custom_acc: 0.5200 - val_loss: 4866.8799 - val_custom_acc: 0.4110\n",
      "Epoch 479/500\n",
      "18s - loss: 4882.6006 - custom_acc: 0.5199 - val_loss: 4866.9926 - val_custom_acc: 0.4109\n",
      "Epoch 480/500\n",
      "18s - loss: 4882.5375 - custom_acc: 0.5203 - val_loss: 4867.0361 - val_custom_acc: 0.4107\n",
      "Epoch 481/500\n",
      "18s - loss: 4882.3607 - custom_acc: 0.5204 - val_loss: 4867.1075 - val_custom_acc: 0.4105\n",
      "Epoch 482/500\n",
      "18s - loss: 4882.2248 - custom_acc: 0.5207 - val_loss: 4867.2036 - val_custom_acc: 0.4105\n",
      "Epoch 483/500\n",
      "18s - loss: 4882.1080 - custom_acc: 0.5210 - val_loss: 4867.2002 - val_custom_acc: 0.4102\n",
      "Epoch 484/500\n",
      "18s - loss: 4882.0137 - custom_acc: 0.5211 - val_loss: 4867.3577 - val_custom_acc: 0.4104\n",
      "Epoch 485/500\n",
      "17s - loss: 4881.7460 - custom_acc: 0.5217 - val_loss: 4867.1838 - val_custom_acc: 0.4106\n",
      "Epoch 486/500\n",
      "18s - loss: 4882.2179 - custom_acc: 0.5210 - val_loss: 4867.2666 - val_custom_acc: 0.4106\n",
      "Epoch 487/500\n",
      "18s - loss: 4881.8553 - custom_acc: 0.5218 - val_loss: 4867.2827 - val_custom_acc: 0.4106\n",
      "Epoch 488/500\n",
      "17s - loss: 4881.6542 - custom_acc: 0.5221 - val_loss: 4867.5772 - val_custom_acc: 0.4097\n",
      "Epoch 489/500\n",
      "17s - loss: 4881.4205 - custom_acc: 0.5222 - val_loss: 4867.3124 - val_custom_acc: 0.4107\n",
      "Epoch 490/500\n",
      "18s - loss: 4881.5397 - custom_acc: 0.5223 - val_loss: 4867.5006 - val_custom_acc: 0.4099\n",
      "Epoch 491/500\n",
      "17s - loss: 4881.2350 - custom_acc: 0.5223 - val_loss: 4867.7006 - val_custom_acc: 0.4092\n",
      "Epoch 492/500\n",
      "18s - loss: 4881.0925 - custom_acc: 0.5228 - val_loss: 4867.4551 - val_custom_acc: 0.4105\n",
      "Epoch 493/500\n",
      "18s - loss: 4881.3021 - custom_acc: 0.5227 - val_loss: 4867.7339 - val_custom_acc: 0.4100\n",
      "Epoch 494/500\n",
      "18s - loss: 4880.9370 - custom_acc: 0.5232 - val_loss: 4867.8539 - val_custom_acc: 0.4091\n",
      "Epoch 495/500\n",
      "18s - loss: 4880.7843 - custom_acc: 0.5235 - val_loss: 4868.0709 - val_custom_acc: 0.4096\n",
      "Epoch 496/500\n",
      "18s - loss: 4880.8316 - custom_acc: 0.5233 - val_loss: 4868.1935 - val_custom_acc: 0.4092\n",
      "Epoch 497/500\n",
      "17s - loss: 4880.6050 - custom_acc: 0.5237 - val_loss: 4868.2700 - val_custom_acc: 0.4082\n",
      "Epoch 498/500\n",
      "17s - loss: 4880.8693 - custom_acc: 0.5233 - val_loss: 4868.0627 - val_custom_acc: 0.4095\n",
      "Epoch 499/500\n",
      "18s - loss: 4880.2465 - custom_acc: 0.5246 - val_loss: 4868.0893 - val_custom_acc: 0.4089\n",
      "Epoch 500/500\n",
      "18s - loss: 4880.7333 - custom_acc: 0.5237 - val_loss: 4868.1452 - val_custom_acc: 0.4096\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "# verbose: 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "hist = model.fit(train_x, train_y, nb_epoch=EPOCHS, shuffle=False, verbose=2,\n",
    "                 validation_data=(test_x, test_y))\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xc7a39b0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFkCAYAAACJu/k0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmcXHWd7//Xmw6dlWxkhSQQCEsggkkjiyCDhmtUHEcH\nL9LAeHVmdERlmNwfV8eH1wEdx1GZAa6A94HEUUagvYor4hgJCkEIRNOIBBJAlgSyr50NsnR/fn98\nT9knle6kq9PVVal+Px+P86iq8/2eU986xvDOdzlHEYGZmZlZuRxW6QaYmZlZbXPYMDMzs7Jy2DAz\nM7OyctgwMzOzsnLYMDMzs7Jy2DAzM7OyctgwMzOzsnLYMDMzs7Jy2DAzM7OyctgwMzOzsiopbEi6\nVlJb0fZMUfkSSdskbZR0v6Qzi87RX9KtktZL2irpHkljiuqMkHSXpBZJmyTNkTT44H6qmZmZVUJ3\nejYWA2OBcdl2Xq7sWeATwDTgXOBl4JeSjszVuQm4CLgYOB84CvhB0XfcDUwFZmZ1zwdu60ZbzczM\nrMJUyoPYJF0L/EVEzOhi/SOAFmBmRPxa0lBgHXBpRPwoq3MSsAQ4OyIWSpoKPA00RMQTWZ1ZwH3A\nhIhY3fWfZ2ZmZpXWnZ6NEyStkPSCpDslTeyokqTDgb8DNgNPZrsbgH7AA4V6EfEssBw4J9t1NrCp\nEDQy84AAzupGe83MzKyC+pVY/zHgQ6ThkvHAdcB8SdMiYjuApIuA7wKDgJXAf4uIjdnx44BdEbGl\n6LxrsrJCnbX5woholbQxV2cf2VDNLNLQzesl/i4zM7O+bABwLDA3Ijb09MlLChsRMTf3cbGkhcAy\n4BLgW9n+XwGnA6OAjwDfl3RmRKzvgfbuzyzgrjJ/h5mZWS27nDRvskeV2rOxl4hokfQcMCW37zXg\nxWxbmJX/DfAVYDVQL2loUe/G2KyM7LV4dUodMDJXpyMvA9x5551MnTr1YH6WlWD27NnceOONlW5G\nn+Jr3vt8zXufr3nvWrJkCVdccQVk/y3taQcVNiQNIQWN/9xPtcOA/tn7RcAe0iqT/ATRScCCrM4C\nYLik6bl5GzMBAY/v53teB5g6dSozZnRp/qr1gGHDhvl69zJf897na977fM0rpizTEEoKG5KuB+4l\nDZ0cDXwe2A00SRoEfBb4KbCKNIzySdLS1u8DRMQWSd8EbpC0CdgKfA14JCIWZnWWSpoL3C7pSqAe\nuBlo8koUMzOzQ0+pPRsTSGM5R5KWsP6GtGR1g6T+wMnAB0lBYwPwW+C8iFiSO8dsoBW4h9Tj8QvS\nvTnyLgNuIa1CacvqXl1iW83MzKwKlDpBtHE/ZTtJN+o60Dl2AldlW2d1NgNXlNI2MzMzq05+Nood\nlMbGTvOnlYmvee/zNe99vua1paQ7iFYzSTOARYsWLfKkIjMzsxI0NzfT0NAA6e7dzT19fvdsmJmZ\nWVk5bJiZmVlZOWyYmZlZWTlsmJmZWVk5bJiZmVlZOWyYmZlZWTlsmJmZWVk5bJiZmVlZOWyYmZlZ\nWTlsmJmZWVk5bJiZmVlZOWyYmZlZWTlsmJmZWVk5bJiZmVlZOWyYmZlZWTlsmJmZWVk5bJiZmVlZ\nOWyYmZlZWTlsmJmZWVk5bJiZmVlZOWyYmZlZWZUUNiRdK6mtaHsmK+sn6SuS/iBpm6QVku6QNL7o\nHA8WHd8q6etFdUZIuktSi6RNkuZIGnzwP9fMzMx6W79uHLMYmAko+7wnex0EvBH4PPAHYATwNeAn\nwJm54wP4BvC53Dl2FH3H3cDY7HvqgW8DtwFXdKO9ZmZmVkHdCRt7ImJd8c6I2ALMyu+T9EngcUkT\nIuLVXNGOjs6RHXNydp6GiHgi23cVcJ+kayJidTfabGZmZhXSnTkbJ2RDJC9IulPSxP3UHU7qydhc\ntP9ySeskPSXpS5IG5srOATYVgkZmXnaes7rRXjMzM6ugUns2HgM+BDwLjAeuA+ZLmhYR2/MVJfUH\nvgzcHRHbckV3AcuAlcBpwFeBE4H3Z+XjgLX5c0VEq6SNWZmZmZkdQkoKGxExN/dxsaSFpOBwCfCt\nQoGkfsD3Sb0RHy86x5zcx6clrQJ+JWlyRLxUYvvNzMysynVnzsafRESLpOeAKYV9uaAxEXhbUa9G\nRxZmr1OAl4DVwJh8BUl1wMisbL9mz57NsGHD9trX2NhIY2PjgQ41MzOreU1NTTQ1Ne21r6Wlpazf\nqYjo/sHSEGA58E8RcUsuaBwHvDUiNnbhHOcC84HTI2JxNkH0aeCM3ATRtwM/ByZ0NkFU0gxg0aJF\ni5gxY0a3f5OZmVlf09zcTENDA6TFGc09ff6SejYkXQ/cSxo6OZq0zHU30JQFjR+Qlr++Gzhc0tjs\n0I0RsVvSccBlpOCwATgduAF4KCIWA0TEUklzgdslXUla+noz0OSVKGZmZoeeUodRJpDugXEksA74\nDXB2RGyQdAwpZAD8PnsVad7GW0m9F7uAC4GrgcHAK6SekH8p+p7LgFtIq1DagHuyY8zMzOwQU+oE\n0U4nPkTEMqDuAMe/ClzQhe/ZjG/gZWZmVhP8bBQzMzMrK4cNMzMzKyuHDTMzMysrhw0zMzMrK4cN\nMzMzKyuHDTMzMysrhw0zMzMrK4cNMzMzKyuHDTMzMysrhw0zMzMrK4cNMzMzKyuHDTMzMysrhw0z\nMzMrK4cNMzMzKyuHDTMzMyurmgsbEZVugZmZmeU5bJiZmVlZ1VzYaGurdAvMzMwsr+bChns2zMzM\nqkvNhQ33bJiZmVUXhw0zMzMrq5oLGx5GMTMzqy41Fzbcs2FmZlZdai5suGfDzMysupQUNiRdK6mt\naHsmK+sn6SuS/iBpm6QVku6QNL7oHP0l3SppvaStku6RNKaozghJd0lqkbRJ0hxJg7vSRocNMzOz\n6tKdno3FwFhgXLadl+0fBLwR+DwwHXgfcBLwk6LjbwIuAi4GzgeOAn5QVOduYCowM6t7PnBbVxrn\nYRQzM7Pq0q8bx+yJiHXFOyNiCzArv0/SJ4HHJU2IiFclDQX+Grg0Ih7K6nwYWCLpzIhYKGlqdp6G\niHgiq3MVcJ+kayJi9f4a554NMzOz6tKdno0TsiGSFyTdKWnifuoOBwLYnH1uIAWcBwoVIuJZYDlw\nTrbrbGBTIWhk5mXnOetAjXPPhpmZWXUpNWw8BnyI1PPwMWAyML+j+RSS+gNfBu6OiG3Z7nHArqwX\nJG9NVlaoszZfGBGtwMZcnU45bJiZmVWXkoZRImJu7uNiSQuBZcAlwLcKBZL6Ad8n9UZ8vAfa2WXX\nXjubW28dtte+xsZGGhsbe7MZZmZmVampqYmmpqa99rW0tJT1O7szZ+NPIqJF0nPAlMK+XNCYCLwt\n16sBsBqolzS0qHdjbFZWqFO8OqUOGJmr06l/+qcbede7ZnTn55iZmdW8jv4B3tzcTENDQ9m+86Du\nsyFpCClorMo+F4LGccDMiNhUdMgiYA9plUnhHCcBk4AF2a4FwHBJ03PHzQQEPH6gNnmCqJmZWXUp\nqWdD0vXAvaShk6NJy1x3A01Z0PgBafnru4HDJY3NDt0YEbsjYoukbwI3SNoEbAW+BjwSEQsBImKp\npLnA7ZKuBOqBm4GmA61EAc/ZMDMzqzalDqNMIN0D40hgHfAb4OyI2CDpGFLIAPh99irSvI23AvOz\nfbOBVuAeoD/wC+ATRd9zGXALaRVKW1b36q400GHDzMysupQ6QbTTWZYRsQyo68I5dgJXZVtndTYD\nV5TStvZju3OUmZmZlUvNPRvFPRtmZmbVpebChns2zMzMqkvNhQ33bJiZmVUXhw0zMzMrq5oLGx5G\nMTMzqy41Fzbcs2FmZlZdai5suGfDzMysutRc2HDPhpmZWXVx2DAzM7Oyqrmw4WEUMzOz6lJzYcM9\nG2ZmZtXFYcPMzMzKqubChodRzMzMqkvNhQ33bJiZmVWXmgsb7tkwMzOrLjUXNtyzYWZmVl0cNszM\nzKysai5seBjFzMysutRc2HDPhpmZWXWpubDhng0zM7PqUnNhwz0bZmZm1cVhw8zMzMqq5sKGh1HM\nzMyqS0lhQ9K1ktqKtmdy5e+TNFfS+qzstA7O8WDR8a2Svl5UZ4SkuyS1SNokaY6kwV1po3s2zMzM\nqku/bhyzGJgJKPu8J1c2GHgY+H/A7Z0cH8A3gM/lzrGjqM7dwNjse+qBbwO3AVccqHHu2TAzM6su\n3QkbeyJiXUcFEXEngKRjaA8SHdnR2TkknQzMAhoi4ols31XAfZKuiYjV+2ucezbMzMyqS3fmbJwg\naYWkFyTdKWliN85xuaR1kp6S9CVJA3Nl5wCbCkEjM4/UI3LWgU7ssGFmZlZdSu3ZeAz4EPAsMB64\nDpgvaVpEbO/iOe4ClgErgdOArwInAu/PyscBa/MHRESrpI1Z2X55GMXMzKy6lBQ2ImJu7uNiSQtJ\nweES4FtdPMec3MenJa0CfiVpckS8VEp7OuKeDTMzs+rSnTkbfxIRLZKeA6YcxGkWZq9TgJeA1cCY\nfAVJdcDIrGy/7r57NosWDdtrX2NjI42NjQfRRDMzs9rQ1NREU1PTXvtaWlrK+p0HFTYkDSGFhP/s\noLirAxrTs7qrss8LgOGSpufmbRRWvzx+oJN94AM38oUvzOjiV5uZmfUtHf0DvLm5mYaGhrJ9Z0lh\nQ9L1wL2koZOjgc8Du4GmrHwEMCkrE3CyJAGrI2KNpOOAy4CfAxuA04EbgIciYjFARCyVNBe4XdKV\npKWvNwNNB1qJAh5GMTMzqzalrkaZQLoHxlLgu8A64OyI2JCVvwd4ghRIghRCmoG/y8p3ARcCc4El\nwPXA97Pj8i7LvmMe8DNgfu4c++UJomZmZtWl1Ami+534EBF3AHfsp/xV4IIufM9munADr464Z8PM\nzKy6+NkoZmZmVlY1Fzbcs2FmZlZdHDbMzMysrGoubHgYxczMrLrUXNhwz4aZmVl1cdgwMzOzsqq5\nsOFhFDMzs+pSc2HDPRtmZmbVpebChns2zMzMqkvNhQ33bJiZmVUXhw0zMzMrq5oLGx5GMTMzqy41\nFzbcs2FmZlZdai5suGfDzMysutRc2HDPhpmZWXVx2DAzM7Oyqrmw4WEUMzOz6lJzYcM9G2ZmZtWl\n5sKGezbMzMyqS82FDfdsmJmZVReHDTMzMyurmgsbHkYxMzOrLjUXNtyzYWZmVl1qLmy4Z8PMzKy6\nlBQ2JF0rqa1oeyZX/j5JcyWtz8pO6+Ac/SXdmtXZKukeSWOK6oyQdJekFkmbJM2RNLgrbXTPhpmZ\nWXXpTs/GYmAsMC7bzsuVDQYeBj4FdNbHcBNwEXAxcD5wFPCDojp3A1OBmVnd84HbutI4hw0zM7Pq\n0q8bx+yJiHUdFUTEnQCSjgFUXC5pKPDXwKUR8VC278PAEklnRsRCSVOBWUBDRDyR1bkKuE/SNRGx\nen+N8zCKmZlZdelOz8YJklZIekHSnZImlnBsAyngPFDYERHPAsuBc7JdZwObCkEjM4/UU3LWgb7A\nPRtmZmbVpdSw8RjwIVLPw8eAycD8rs6nIA277IqILUX712RlhTpr84UR0QpszNXplHs2zMzMqktJ\nwygRMTf3cbGkhcAy4BLgWz3ZsO763e9m8573DNtrX2NjI42NjRVqkZmZWfVoamqiqalpr30tLS1l\n/c7uzNn4k4hokfQcMKWLh6wG6iUNLerdGJuVFeoUr06pA0bm6nRq+vQb+elPZ3SxOWZmZn1LR/8A\nb25upqGhoWzfeVD32ZA0hBQ0VnVQ3NGAxiJgD2mVSeEcJwGTgAXZrgXAcEnTc8fNJE04ffxAbfKc\nDTMzs+pSUs+GpOuBe0lDJ0cDnwd2A01Z+QhScDiaFA5OliRgdUSsiYgtkr4J3CBpE7AV+BrwSEQs\nBIiIpZLmArdLuhKoB24Gmg60EiUdX8ovMjMzs3IrdRhlAukeGEcC64DfAGdHxIas/D2kuRuRbYVB\noc8DX8jezwZagXuA/sAvgE8Ufc9lwC2kVShtWd2ru9JA92yYmZlVl1IniO53lmVE3AHccYA6O4Gr\nsq2zOpuBK0ppW8Hu3d05yszMzMql5p6N4rBhZmZWXWoubOzcWekWmJmZWZ7DhpmZmZVVzYWNXbsq\n3QIzMzPLq7mw4Z4NMzOz6lJzYcM9G2ZmZtWl5sKGezbMzMyqS82FDfdsmJmZVZeaCxvu2TAzM6su\nNRc2Wlthz55Kt8LMzMwKai5sgHs3zMzMqklNho3XX690C8zMzKzAYcPMzMzKymHDzMzMyqomw8Zr\nr1W6BWZmZlZQk2HDPRtmZmbVw2HDzMzMysphw8zMzMrKYcPMzMzKymHDzMzMysphw8zMzMqqJsOG\nl76amZlVj5oLG/X17tkwMzOrJiWFDUnXSmor2p4pqvMFSSsl7ZB0v6QpReUPFh3fKunrRXVGSLpL\nUoukTZLmSBrclTY6bJiZmVWX7vRsLAbGAuOy7bxCgaRPA58EPgqcCWwH5kqqzx0fwDdy5xgPfKro\nO+4GpgIzgYuA84HbutK4AQMcNszMzKpJv24csyci1nVSdjXwzxHxMwBJHwTWAO8Fvpert6Ozc0g6\nGZgFNETEE9m+q4D7JF0TEav31zj3bJiZmVWX7vRsnCBphaQXJN0paSKApMmknooHChUjYgvwOHBO\n0Tkul7RO0lOSviRpYK7sHGBTIWhk5pF6RM46UOMcNszMzKpLqT0bjwEfAp4lDX9cB8yXNI0UNILU\nk5G3JisruAtYBqwETgO+CpwIvD8rHweszZ8gIlolbSw6T4f69/dqFDMzs2pSUtiIiLm5j4slLSQF\nh0uApV08x5zcx6clrQJ+JWlyRLxUSns60r8/7NhxsGcxMzOzntKdORt/EhEtkp4DpgAPAiJN/Mz3\nbowFntj36D9ZmL1OAV4CVgNj8hUk1QEjs7L9euWV2axbN4z3vKd9X2NjI42NjQc61MzMrOY1NTXR\n1NS0176WlpayfudBhQ1JQ0gh4Y6IeEnSatIKkj9k5UNJ8yxu3c9pppOGX1ZlnxcAwyVNz83bmEkK\nMo8fqE3Tp9/I4YfP4Kc/7c4vMjMzq20d/QO8ubmZhoaGsn1nSWFD0vXAvaShk6OBzwO7ge9mVW4C\n/rekPwIvA/8MvAr8JDv+OOAy4OfABuB04AbgoYhYDBARSyXNBW6XdCVQD9wMNB1oJQrAoEGwaVMp\nv8rMzMzKqdSejQmke2AcCawDfgOcHREbACLiq5IGke6JMRx4GHhnROzKjt8FXEhaIjsYeAX4PvAv\nRd9zGXALaRVKG3BPdswBDRoEy5eX+KvMzMysbEqdIHrAiQ8RcR1plUpHZa8CF3ThHJuBK0ppW8Gg\nQbBtW3eONDMzs3KouWejDBoEW7dWuhVmZmZWUJNhwz0bZmZm1aMmw8Zrr8GePZVuiZmZmUGNhg2A\n7dsr2w4zMzNLajZseCjFzMysOtRc2BiYPdLNk0TNzMyqQ82FjcGD06t7NszMzKpDzYWNwjCKezbM\nzMyqQ82GDfdsmJmZVYeaDRvu2TAzM6sONRc2BgwAyT0bZmZm1aLmwoYEQ4Y4bJiZmVWLmgsbkMKG\nh1HMzMyqQ02GjaFDYcuWSrfCzMzMoEbDxqhRsH59pVthZmZm4LBhZmZmZVaTYWP0aFi3rtKtMDMz\nM6jRsOGeDTMzs+pRk2HDPRtmZmbVo2bDxrZt8PrrlW6JmZmZ1WTYGDUqvXooxczMrPJqMmyMHp1e\nPZRiZmZWeTUZNtyzYWZmVj1qMmy4Z8PMzKx6lBQ2JF0rqa1oe6aozhckrZS0Q9L9kqYUlfeXdKuk\n9ZK2SrpH0piiOiMk3SWpRdImSXMkDe5qOwcPhoEDHTbMzMyqQXd6NhYDY4Fx2XZeoUDSp4FPAh8F\nzgS2A3Ml1eeOvwm4CLgYOB84CvhB0XfcDUwFZmZ1zwduK6WR48fDqlWlHGFmZmbl0K8bx+yJiM76\nDK4G/jkifgYg6YPAGuC9wPckDQX+Grg0Ih7K6nwYWCLpzIhYKGkqMAtoiIgnsjpXAfdJuiYiVnel\nkZMmwfLl3fh1ZmZm1qO607NxgqQVkl6QdKekiQCSJpN6Oh4oVIyILcDjwDnZrjNIASdf51lgea7O\n2cCmQtDIzAMCOKurjZw0CZYtK/WnmZmZWU8rNWw8BnyI1PPwMWAyMD+bTzGOFAjWFB2zJiuDNPyy\nKwshndUZB6zNF0ZEK7AxV+eA3LNhZmZWHUoaRomIubmPiyUtBJYBlwBLe7Jh3TV79myGDRvG8uXw\n6qvw7nfD5Zc30tjYWOmmmZmZVVxTUxNNTU177WtpaSnrd3ZnzsafRESLpOeAKcCDgEi9F/nejbFA\nYUhkNVAvaWhR78bYrKxQp3h1Sh0wMlenUzfeeCMzZszgl7+EWbPgllvg2GNL/mlmZmY1qbFx33+A\nNzc309DQULbvPKj7bEgaQgoaKyPiJVIYmJkrH0qaZ/FotmsRsKeozknAJGBBtmsBMFzS9NxXzSQF\nmce72rZJk9Krh1LMzMwqq6SeDUnXA/eShk6OBj4P7Aa+m1W5Cfjfkv4IvAz8M/Aq8BNIE0YlfRO4\nQdImYCvwNeCRiFiY1VkqaS5wu6QrgXrgZqCpqytRACZOTK+eJGpmZlZZpQ6jTCDdA+NIYB3wG+Ds\niNgAEBFflTSIdE+M4cDDwDsjYlfuHLOBVuAeoD/wC+ATRd9zGXALaRVKW1b36lIaOngwHHUULK2K\nmSRmZmZ9V6kTRA84yzIirgOu20/5TuCqbOuszmbgilLa1pFp0+Dppw/2LGZmZnYwavLZKAWnnuqw\nYWZmVmk1HzZeeAF27Kh0S8zMzPqumg4b06ZBhOdtmJmZVVJNh41TTgEJnnjiwHXNzMysPGo6bBxx\nBLzhDbBgwYHrmpmZWXnUdNgAePOb4dFHD1zPzMzMyqPmw8a558KSJbBxY6VbYmZm1jf1ibABMH9+\nZdthZmbWV9V82Jg8GU48EX7+80q3xMzMrG+q+bABcNFFKWxEVLolZmZmfU+fCBvvehesWOElsGZm\nZpXQJ8LGBRfA6NFw992VbomZmVnf0yfCRr9+cOml0NQEra2Vbo2ZmVnf0ifCBsAHPwgrV8J991W6\nJWZmZn1LnwkbZ5wBZ58NN91U6ZaYmZn1LX0mbADMng2//rVvX25mZtab+lTYuPji9Nj5z37Wy2DN\nzMx6S58KG3V18OUvp96NpqZKt8bMzKxv6FNhA+Dd74YPfACuugrWrKl0a8zMzGpfnwsbADffDIcd\nBh/7GLS1Vbo1ZmZmta1Pho3Ro+H22+HHP4Zrr610a8zMzGpbnwwbAO99b5q/8cUvwpw5lW6NmZlZ\n7epX6QZU0qc+BcuWwUc+Ajt2wN//faVbZGZmVnsOqmdD0j9KapN0Q27fGEnflrRC0nZJP5c0pei4\nB7PjClurpK8X1Rkh6S5JLZI2SZojafDBtHff9sOtt8I118DVV8P/+l+wZ09PfoOZmZl1O2xIehPw\nUeDJoqKfAMcCfw68EVgOzJM0MFcngG8AY4FxwHjgU0XnuRuYCswELgLOB27rbns7I8H118ONN6bt\nbW+DV17p6W8xMzPru7oVNiQNAe4E/hbYnNt/AnAW8LGIaI6I54ErgYFAY9FpdkTEuohYm23bcuc5\nGZgF/E1E/C4iHgWuAi6VNK47bT6Qf/gHeOgheOklOOWUtGLFvRxmZmYHr7s9G7cC90bEr4r29yf1\nWuws7IiIwufziupeLmmdpKckfamo5+McYFNEPJHbNy8791ndbPMBnXsuLF4Mf/VXaf7GG94A99zj\n5bFmZmYHo+SwIelS0vDIZzooXgq8AvyrpOGS6iV9GphAGiopuAu4ArgA+BLwV8B3cuXjgLX5E0dE\nK7AxKyubYcPg61+H3/0OjjkG/vt/Tw9x+853YOfOAx9vZmZmeyspbEiaANwEXB4Ru4vLI2IP8D7g\nRFIw2Ab8GfBzoC1Xb05E3B8RT0dEEyls/KWkyd3+JT2soQF+8Qt48EEYNSo9on7SJPjMZ+Cppyrd\nOjMzs0NHqUtfG4DRQLMkZfvqgPMlfRLonw19zJB0BFAfERskPQb8dj/nXZi9TgFeAlYDY/IVJNUB\nI7OyTs2ePZthw4btta+xsZHGxuIpI13zZ3+WtqVL08qV225L9+c49VS49FL4y7+EqVPTRFMzM7Nq\n19TURFPRA8JaWlrK+p2KEh5/mi09PaZo97eBJcCXI2JJB8eckJXPiogHOjnvucB84PSIWJxNEH0a\nOKMwb0PS20k9JBMiYp/AIWkGsGjRokXMmDGjy7+pVLt2wf33pwe5/eQnsG0bTJwI73gHzJqVgsmo\nUWX7ejMzsx7X3NxMQ0MDQENENPf0+Uvq2YiI7cAz+X2StgMbCkFD0vuBdaQlr6eRhl1+WAgako4D\nLiMFhw3A6cANwEMRsTj7nqWS5gK3S7oSqAduBpo6Chq9qb4eLrooba+9BvPnp+GWX/wi3QId4OST\n4S1vgfPOgze/GY4/3j0fZmbWd/XEHUSLu0bGk8LDGGAVcAfwxVz5LuBC4GpgMGlC6feBfyk6z2XA\nLaRVKG3APdkxVWPgwNSbMWtWukfH8uXw8MPwm9+krRA+hg+HGTPS1tCQtuOPTw+DMzMzq3UlDaNU\ns94aRinFxo2wcCEsWtS+LV+eyoYOTUtr3/AGmDat/XXkyMq22czM+p6qGkax0owcmeZyvOMd7fvW\nr4fm5hQ8nnoq9YDMmdN+A7Gjjto7fEybliagDu7RG7WbmZn1HoeNXjZqFLz97Wkr2LULnn8+hY/F\ni9Prj34E//7v7XUmTICTTkrzQU46qX2bONHDMWZmVt0cNqpAfX1aSnvqqXvv37YNnnkGliyBZ59N\n24MPprkgu3alOgMHwgkn7B1ApkyBY4+FsWM9MdXMzCrPYaOKDRkCZ56ZtrzWVli2rD2AFLZHHoGV\nK9vrDRiQ7oJ67LHtr4XtmGNg3Dj3ipiZWfk5bByC6urguOPS9s537l22dSu8+CK8/HIKJC+/nLbf\n/jY952Xjxva69fX7hpGJE2H8+PZt5Ej3jpiZ2cFx2KgxRxwBp5+eto5s2dIeQvJh5Ikn0jyRDRv2\nrt+/f+rVJbraAAATtElEQVQBGT8+TV7NB5GjjkpDNaNGpW3gwA6+0MzM+jyHjT4mv+S2I6+/DqtW\n7b2tXNn+fv789Lp+/b7HDhoEo0e3h4/9bUceme4/MmCAe07MzGqdw4btZcAAmDw5bfuzaxesXg1r\n16bg0dH26qvw+9+3f25t3fc89fUwYkQKHoXX/Pv97Rs2DPr5T7CZWdXzX9XWLfX16Sm4kyZ1rX5b\nWxrCWb8e1q1Lc0c2b4ZNm9Jr/v2aNWnCa+FzSwt0du+5I45IvTXDhqXXwlb8eX/7hgzxRFkzs3Jy\n2LBecdhh7T0UU6aUdmwhqOQDSf51y5b2raUlba+8sve+bdv2/x2F0HKgoDJsWHuvSvFrfX33r4+Z\nWS1z2LCqlw8qxx7bvXO0tqbAkQ8g+ZDS2b5SQsvAgZ0Hkfxr4f2IEWmOy+jR6bPnrphZrXLYsD6h\nri79B33YsIM7T2trey9LYYino9fC+3Xr4I9/3LuscGv6vH790sTZQvgoTLTt6P3o0WmCreermNmh\nwn9dmZWgri71SIwY0b3jI2DHjhQ8Nm5MYaQwj6WwFT4/80z7+44CSr5npKOgMnZs+7yaQYMO7neb\nmR0Mhw2zXiSlh+oNHpzuU9IVESmcdBZKCu+ffLL9ffFwz6hRKXQcc0zHr6NHexjHzMrHYcOsykml\nT6597bW0qmf58rQtW9b++stfptcdO9rrDxjQ3gtSCCD59xMnegKsmXWfw4ZZDRo4sP05OB2JSMM4\n+SBSeP/UU/Czn6WwUiClO8l2FEQKr8OHu3fEzDrmsGHWB0lpkumRR8L06R3Xef31tBqno96R5ub0\nvvD0YUj3K+lsmGbSpDRs5EmtZn2T/69vZh0aMABOOCFtHWlrS3eQLQ4iy5fDY4/B976394P/6urg\n6KP3DiLHHQfHH59ejz461TGz2uOwYWbdcthhaWhl3Dg488yO62zbtm/PSOH9/PnplvaFu8PW16dh\nn8ITjfNBZPLkdOM1Mzs0OWyYWdkMGQKnnJK2juzcmYLHiy/CCy+k1xdfhIcfhjvugO3b2+uOGpXC\nx4knpt6W449PE2aPPx5GjvR8EbNq5rBhZhXTv38KDyeeuG9ZRFrK+8IL8NJLKYT88Y/w/PPwX/+1\n95OHhw1rDx75EHL88WmuiJ99Y1ZZDhtmVpUkGDMmbeecs2/5li0piPzxj+m18H7Bgr2HZwYMaB+S\nKQ4kxxwDhx/eu7/LrC9y2DCzQ9LQoWklTUeraV5/PfWGFIeRn/0MXn4Zdu9O9erqUuDoqEfk+ON9\n51WznuKwYWY1Z8AAmDo1bcVaW9OS3uIekUcfhe98Z+95IuPHdxxChg5N+7yU16xrDur/KpL+EfgS\ncFNE/M9s3xjgq8B/A4YDDwF/HxF/zB3XH7gB+ADQH5gLfDwi1ubqjABuAd4NtAE/AK6OiNxfBWZm\npamra7/h2YUX7l0WkW5mlg8hL7wAS5bAvffChg3tdQu3gB8yJJ2rtRXe/vb2IZ+jj3bPiFlBt8OG\npDcBHwWeLCr6CbAT+HNgK/D/AfMkTY2I17I6NwHvBC4GtgC3ksLEW3LnuRsYC8wE6oFvA7cBV3S3\nzWZm+1O4U+q4cXDuufuWt7Sk8LFpE/z61yl8bN4Mzz6bHpZ3113tdfv1gzPOSLd637MH3vWuNJF1\n584USsaM6b3fZVZp3QobkoYAdwJ/C3wut/8E4CzglIhYmu27ElgNNAL/IWko8NfApRHxUFbnw8AS\nSWdGxEJJU4FZQENEPJHVuQq4T9I1EbG6ez/XzKz7hg2DGTPS+5kz9y1fsSL1gvTrl17nz0+BZOdO\n+MhH2usVnnfTrx+89a2pB2TaNDj9dDjttDQMNGhQqucbnVkt6G7Pxq3AvRHxK0mfy+3vDwSpZwOA\niAhJO4HzgP8Azsi+94FcnWclLQfOARYCZwObCkEjMy8791mk3hMzs6py9NFpA7jgArjyyvay555L\nYaWuDu67D1atSndYXbAghZHvfW/vh+MNHpwmuk6fnoZpTj4Z3vQmGDs2DeGMHZtW0vTv35u/0Kx7\nSg4bki4F3kgKDcWWAq8A/yrpY8AOYDYwARif1RkL7IqILUXHrgHGZe/HAWvzhRHRKmljro6Z2SEj\nfy+R//E/9i1va0tDNH/4Q3rmzCuvpDAxf34atrntNvjiF/c+pr4+zRE5/PAUPs4+G0aMSGHktNPS\n8M2ECb7hmVVeSWFD0gTSfIsLI2J3cXlE7JH0PuCbwEZgD6lH4udAr/xxnz17NsOGDdtrX2NjI42N\njb3x9WZm3XLYYR0/i2b27PQaAStXpmGZNWvSTc3WrYMHHkih4qGHUu/I7qK/madPT70pu3al3pYR\nI9LrlCkp0Bx5ZMc3VbPa1dTURFNT0177WlpayvqdisKdb7pSWfoL4IdAK+3hoY40vNEK9I/shJKO\nAOojYoOkx4DfRsRVkt5KCiAj8r0bkl4GboyI/5PN4fi3iDgyV14HvA68PyL2GUaRNANYtGjRImYU\nBlXNzPqItrb0umVLGp757W9TCPnRj9L8kJ074fHHU1jJPyAP0k3Ppk9PPSDbt8Ob35z2nXhi6jFx\n70jta25upqGhAdJcyeaePn+pwyjzgDcU7fs2sAT4cuSSS0RshT9NGj0D+GxWtIjU4zET+FFW5yRg\nErAgq7MAGC5pem7exkxSwHm8xDabmdW8wi3Zhw9P23HHpc+XX753vT17YOnS1KsxalR6MN4jj8CT\nT6YekH794CtfSQ/RKzjiiLTEd8SIFDwmT05B5Nhj01yUqVPT5NYJE3zvEetYSX8ssntcPJPfJ2k7\nsCEilmSf3w+sA5YDp5GGXX4YEQ9k59gi6ZvADZI2kZbHfg14JCIWZnWWSpoL3J6tZqkHbgaavBLF\nzKz7+vVLK1+mTUuf3/QmuPjifeu1tKT7jKxYkZb2vvZaGrp59VV47LF0A7QdO2DgwFQGKaxMnNh+\nH5MJE9KE2fHj03LiAQNSEDrqKN8mvq/piQxaPA4znnTDrjHAKuAOoGhaE7NJwy73kFaw/AL4RFGd\ny0g39ZpHuqnXPcDVPdBeMzM7gGHDoKEhbR0p3ABt9OjUO/L88+kW8S+9lG4Jv2QJ3H9/WnVTGOIp\nOOywFD4mTkzb8OEpuLzjHSmkFMLJkCHl/pXWW0qas1HNPGfDzKz6tLameSKrVqV5Ixs3piGc/LZp\nU6r3ZNEtIocOTWGkcM+Rc8+Fk05KPS4XXJB6TjZvTqtwvAT44FTbnA0zM7Muq6trf3rvgWzenEJJ\nYVuxIg3bvP56Cirf/W7aP2YM/Nu/tR9X6Ck5+ujU0zJkSNqOOy71lIwbl+aYDBqUhn18G/ne57Bh\nZmZVoTC5taMH6BXs2pXmezz3HKxdm0LFb3/bHkw2bkw9KS+/DD/8YQoweYcdlkJJa2vqHVm2LL2/\n+OI0j2XMmHT+5ctTj8mRR3bUCiuVw4aZmR0y6uvT60knpQ3Sst3ObN+e7k/y/POpd2T9+nTztN27\n00TXoUNTveuu2/uJv9A+mbZfv9RjMm5cCibbtsGpp6bvP+aYtB11lG8tvz8OG2ZmVrMGD+74ZmnF\n2tpSb8bGjWnYZvRomDcPnshuvrBuHTzzTJoYe8QR8I1vpAmyBXV1MHJke+9MYRs5MgWRwq3sjzoq\nhZZRo9qXK/cFDhtmZtbnHXZY+5LdggMFlK1bU0BZtiy9Fp4CXNg2bUo9KitXpiGfvLq6dMO0ESPS\n8VOmpDBy8skp6Bx3XFoqfOKJqe7QoYd2QHHYMDMz64YjjkjDKaeeeuC6u3a1T3xdvbp9W7curap5\n8cUUSn784xRMtm7d9xyHH556RcaNa590W9hGj069Jrt3px6XhoY096VahnYcNszMzMqsvr59fseB\ntLamm6pt2ZJ6TNra0ucVK9K2dm3aliyBBx9M74vnm0DqGamra3/+TWGuyeTJaR7K8OEpBJ1ySpq/\nUk4OG2ZmZlWkMP9j5Mi9h3X2Z8eOdM+SXbtSoGluTvct2b07TYp97rk0bPO738FPf5omy27dmkLH\n7t37XwHUExw2zMzMDnGDBrWvzoG0rPeCCzqvH5FCxubNKaREpFvXl8shOtXEzMzMuktKQztjxqT5\nHeWeeOqwYWZmZmXlsGFmZmZl5bBhZmZmZeWwYWZmZmXlsGFmZmZl5bBhZmZmZeWwYWZmZmXlsGFm\nZmZl5bBhZmZmZeWwYWZmZmXlsGFmZmZl5bBhZmZmZeWwYQelqamp0k3oc3zNe5+vee/zNa8tBxU2\nJP2jpDZJN+T2DZZ0i6RXJO2Q9LSkvys67sHsuMLWKunrRXVGSLpLUoukTZLmSBp8MO21nue/EHqf\nr3nv8zXvfb7mtaVfdw+U9Cbgo8CTRUU3AhcAlwHLgLcD/1fSioj4WVYngG8AnwOU7dtRdJ67gbHA\nTKAe+DZwG3BFd9tsZmZmva9bPRuShgB3An8LbC4qPge4IyIejojlETGHFEjOLKq3IyLWRcTabNuW\nO//JwCzgbyLidxHxKHAVcKmkcd1ps5mZmVVGd4dRbgXujYhfdVD2KPAeSUcBSHorcAIwt6je5ZLW\nSXpK0pckDcyVnQNsiogncvvmkXpEzupmm83MzKwCSh5GkXQp8EbgjE6qXEUaInlV0h6gFfhIRDyS\nq3MXaYhlJXAa8FXgROD9Wfk4YG3+pBHRKmljVtaRAQBLliwp9SfZQWhpaaG5ubnSzehTfM17n695\n7/M17125/3YOKMsXRESXN2ACsBqYltv3a+CG3OdrgCXAu4BpwMeBLcDb9nPeC4A2YHL2+TPAkg7q\nrQH+rpNzXEbq+fDmzZs3b968dW+7rJRc0NWt1J6NBmA00CypMLGzDjhf0ieB4cC/AO+NiP/KyhdL\nmk4KIR0NuwAszF6nAC+RAs2YfAVJdcDIrKwjc4HLgZeB10v7WWZmZn3aAOBY9p3y0CNKDRvzgDcU\n7fs2qSfjy6TgcThp6CSvlf3PD5lOSlSrss8LgOGSpufmbcwkrVx5vKMTRMQG0goWMzMzK92j5Tpx\nSWEjIrYDz+T3SdoObIiIJdnnh4B/k3QVaV7GBcAHgX/Iyo8jDXn8HNgAnA7cADwUEYuz71kqaS5w\nu6QrSUtfbwaaIqKzng0zMzOrQt2+z0ZOFH3+APCvpKWxI0mB4zMR8Y2sfBdwIXA1MBh4Bfg+afgl\n7zLgFlJvShtwT3aMmZmZHUKUTa40MzMzKws/G8XMzMzKymHDzMzMyqomwoakT0h6SdJrkh7Lntti\n3SDpLZJ+KmlF9pC893RQ5wuSVmYP2rtf0pSi8v6SbpW0XtJWSfdIGlN8HkskfUbSQklbJK2R9CNJ\nJ3ZQz9e9h0j6mKQnswc9tkh6VNI7iur4epdJRw/xzPb7mvcgSdcWPfS0TVLxIo9eueaHfNiQ9AHg\n34FrSUtonwTmShpV0YYdugYDvyfdjG2fCT2SPg18kvQQvjOB7aTrXZ+rdhNwEXAxcD5wFPCD8jb7\nkPYW0mqrs0iTpw8Hfpm/hb+ve497Bfg0MIN0/6BfAT+RNBV8vcups4d4+pqXzWLSQ03HZdt5hYJe\nvebluFNYb27AY8D/yX0W8CrwqUq37VDfSKuA3lO0byUwO/d5KPAacEnu807gfbk6J2XnOrPSv+lQ\n2IBR2fU6z9e9V6/7BuDDvt5lvcZDgGeBt7Hv3ad9zXv+el8LNO+nvNeu+SHdsyHpcNK/Sh4o7It0\nNeaRHuZmPUjSZFIyzl/vLaQbrRWu9xmkJdX5Os8Cy/H/Jl01nNSrtBF83ctN0mHZM58GAY/6epdV\nhw/x9DUvqxOyYfEXJN0paSL0/jXviftsVNIo0l1L1xTtX0NKX9azxpH+I9jR9S48IG8ssCv7Q9tZ\nHetE9hiAm4DfRERhbNXXvQwkTSPdrXgAsJX0r7dnJZ2Dr3eP0/4f4uk/4+XxGPAhUm/SeOA6YH72\nZ79Xr/mhHjbMas3XgVOAcyvdkD5gKekOxsNIT5z+T0nnV7ZJtUnSBFKIvjAidle6PX1FROSfc7JY\n0kLSjTYvIf357zWH9DAKsJ703JWxRfvH0vkD26z7VpPmxOzveq8G6iUN3U8d64CkW0hPS74gIlbl\ninzdyyAi9kTEixHxRER8ljRh8Wp8vcsh/xDP3ZJ2A38GXC1pF+lfyr7mZRYRLcBzpIee9uqf80M6\nbGQJeRHpIW3An7qhZ1LGB8r0VRFReCJv/noPJa2iKFzvRcCeojonAZNIXdbWgSxo/AXw1ohYni/z\nde81hwH9fb3LovAQzzeSepNOB35HeqzF6RHxIr7mZSdpCClorOz1P+eVni3bA7NtLwF2kB72djJw\nG2lW+ehKt+1Q3EhLX08n/aXQRnqA3unAxKz8U9n1/XPSXx4/Bp4H6nPn+DrwEukhfA3AI8DDlf5t\n1bpl12sTaQns2Nw2IFfH171nr/mXsut9DDCN9DynPcDbfL177X+D4tUovuY9f42vJy1XPQZ4M3A/\nqRfpyN6+5hW/GD10QT8OvExasrMAOKPSbTpUN1LXZhtpeCq//UeuznWkJVM7gLnAlKJz9CfdN2I9\naeLd94Exlf5t1bp1cr1bgQ8W1fN177lrPgd4Mfs7YzXwy0LQ8PXutf8NfpUPG77mZbnGTaRbQbxG\nWkFyNzC5EtfcD2IzMzOzsjqk52yYmZlZ9XPYMDMzs7Jy2DAzM7OyctgwMzOzsnLYMDMzs7Jy2DAz\nM7OyctgwMzOzsnLYMDMzs7Jy2DAzM7OyctgwMzOzsnLYMDMzs7L6/wE+zSJ34gL5bwAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc4a4978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vali_loss = list(hist.history.values())[1]\n",
    "plt.plot(range(EPOCHS), vali_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on cb513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s     \n",
      "[4868.1452114238809, 0.40960564757135592]\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(test_x, test_y)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1, 6300)\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "prediction = model.predict(test_x[i:i+1])\n",
    "print(\"Shape:\", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5534, 6300)\n",
      "(1, 6300)\n"
     ]
    }
   ],
   "source": [
    "train_x1 = train_x.reshape(len(train_x), 700*22)\n",
    "train_y1 = train_y.reshape(len(train_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "prediction1 = prediction.reshape(len(prediction), 700*9)\n",
    "test_x1 = test_x.reshape(len(test_x), RESIDUE_SIZE*NUM_RESIDUES)\n",
    "test_y1 = test_y.reshape(len(test_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "print(train_y.shape)\n",
    "print(prediction.shape)\n",
    "#TWO_D = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:\n",
      "\n",
      "Residues:\n",
      "MFKVYGYDSNIHKCVYCDNAKRLLTVKKQPFEFINIMPEKGVFDDEKIAELLTKLGRDTQIGLTMPQVFAPDGSHIGGFDQLREYFK-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Labels:\n",
      "LEEEEELLTTTSLLHHHHHHHHHHHHTTLLEEEEESLSBTTBLLHHHHHHHHHHHTLSLSSSLLSLEEELTTSLEEESHHHHHHHTL-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Predicted:\n",
      "\n",
      "Residues:\n",
      "MFKVYGYDSNIHKCVYCDNAKRLLTVKKQPFEFINIMPEKGVFDDEKIAELLTKLGRDTQIGLTMPQVFAPDGSHIGGFDQLREYFK-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Labels:\n",
      "LLLEEEELLLTLEEEEETTLLEEEEEHTTLLEEEEELLLLHHHHHHHHHHHHHHHHHHHEEEEEEEEEELLTLLHHHHHHHHEEEEE-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected:\\n\")\n",
    "_ = cullpdb_loader.print_residues(test_x[i], labels=test_y1[i], two_d=True)\n",
    "print(\"\\nPredicted:\\n\")\n",
    "_ = cullpdb_loader.print_residues(test_x[i], labels=prediction1[0], two_d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdscbio]",
   "language": "python",
   "name": "conda-env-sdscbio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
