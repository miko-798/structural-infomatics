{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Q8 Classification -- 3/21/2017\n",
    "\n",
    "In which I finally get a working accuracy metric.\n",
    "<br><br>\n",
    "Results:\n",
    "<br><br>\n",
    "1 conv layer: 30 epochs, lr=0.001, SGD, train acc (filtered): 35.6%, test acc (cb513): 31.6%<br>\n",
    "2 conv layer: 30 epochs, lr=0.001, SGD, train acc: 35.4%, test acc: 31.2% (???)<br>\n",
    "2 conv layer: 10 epochs, lr=0.01,  SGD, train acc: 3%, test acc: 3%<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "import cullpdb_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_RESIDUES = 700  # per protein\n",
    "RESIDUE_SIZE = 22   # size of one hot vector per residue\n",
    "NUM_CLASSES = 9     # 8 + 'NoSeq'\n",
    "\n",
    "FILTERS = 1    # Dimensions output by conv layer\n",
    "                  # equivalently, size of output by each convolutional layer\n",
    "WINDOW_SIZE = 11  # \"scope\" of convolution (ie 11 total residues)\n",
    "\n",
    "TWO_D = True\n",
    "if TWO_D:\n",
    "    INPUT_SHAPE = (NUM_RESIDUES, RESIDUE_SIZE)  # see below\n",
    "else:\n",
    "    INPUT_SHAPE = (NUM_RESIDUES * RESIDUE_SIZE,)\n",
    "OUTPUT_SIZE = NUM_CLASSES*NUM_RESIDUES      # output matrix holding predictions\n",
    "#OUTPUT_SIZE = NUM_CLASSES\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0  # momentum coefficient\n",
    "EPOCHS = 30  # iterations of training, total dataset covered once each epoch\n",
    "LOSS='categorical_crossentropy'\n",
    "OPTIMIZER = optimizers.SGD(lr=LEARNING_RATE, momentum=GAMMA, nesterov=False)\n",
    "\n",
    "SHOW_ACCURACY = True  # set to False for quicker train ops\n",
    "\n",
    "SAVE_FILE = \"models/dense-filtered-3-28\"\n",
    "DATA = \"data/cullpdb+profile_6133.npy.gz\"\n",
    "DATA_FILTERED = \"data/cullpdb+profile_6133_filtered.npy.gz\"\n",
    "DATA_TEST = \"data/cb513+profile_split1.npy.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "<b>Data:</b><br> _x represents input proteins, _y represents target structure classifications (each as one-hot vectors) <br><br>\n",
    "<b>Data Shape:</b><br> First dimension represents number of proteins, second number of residues per protein, and third size of residue or structure vector.<br> For example, train_x is shape (5600, 700, 22): it is an <b>m \\* n \\* p</b> matrix where there are <b>m</b> proteins (each row), <b>n</b> residues per protein (each column), and <b>p</b> sized vectors to represent a single residue or a single structure (each \"slice\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein residues and labels...\n",
      "Loading file from c:\\SANJAY\\CS\\Projects\\sdscbio\\data\\cullpdb+profile_6133_filtered.npy.gz...\n",
      "File Loaded.\n",
      "Loaded protein residues and labels.\n",
      "Reshaping...\n",
      "Reshaped\n",
      "Loading file from c:\\SANJAY\\CS\\Projects\\sdscbio\\data\\cb513+profile_split1.npy.gz...\n",
      "File Loaded.\n",
      "Residues:\n",
      "FDYQTVYFANQYGLRTIELGESEFVDNTLDNQHKXVIKAAWGGGYTNRNNVVINFKVDESLCDNLYFKDTDQPLVPXPASYYTLASDRIAIPKGQIXAGVEVQLTDDFFADEKSISENYVIPLLXTNVQGADSILQGKPVVENPVLTNAGDWSILPQNFVLYAVKYVNPWHGEYLRRGIDHATVAGTSKDIIRHEQFVENDEVVNISTKSXKDNLLTLKTKDESGKDISYTVRLSFAEDGSCTVHSGSQNVVVSGSGKFVSKGEKNSLGGKDRNAIYLDYTVNLTDNNIQLATKDTLVLRTRNVYGGKSLEVVRK-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Labels:\n",
      "LLLLEEELSLSEEEEEEELSLLSSSLLHHHHTTEEEEEEEEESSSSLLSLEEEEEEELGGGGTTLEETTTLLBLEELLGGGEEESLSEEEELTTLSEEEEEEEELHHHHHSGGGGSSLEEEEEEEEEEESSSEELLLEESSSSLLTTLGGGEEELLLSEEEEEEEEELTTLEEEEEEEEEEEEETTEEEEEEELLSSGGGSEEEEEEESSSSEEEEEEEEELTTSLEEEEEEEEEELTTSEEEEEELSTTLEEEEEEEEEEEEETTLGGGSLEEEEEEEEEEEETTTTEEEEEEEEEEEEELLLLSEEELLEELL-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "train_x: (5534, 700, 22)\n",
      "train_y (5534, 6300)\n",
      "test_x: (514, 700, 22)\n",
      "test_y: (514, 6300)\n"
     ]
    }
   ],
   "source": [
    "train = cullpdb_loader.load_residues(DATA_FILTERED, split=False, two_d=TWO_D)  # load from my helper file\n",
    "\n",
    "test = cullpdb_loader.load_cb513(DATA_TEST, two_d=TWO_D)\n",
    "\n",
    "# train, validation, and test were loaded as tuples of (input, output);\n",
    "# here we unpack them into separate variables:\n",
    "train_x, train_y = train\n",
    "#train_x, train_y = train_x[:2], train_y[:2]\n",
    "#vali_x, vali_y = validation\n",
    "test_x, test_y = test\n",
    "#test_x, test_y = test_x[:20], test_y[:20]\n",
    "\n",
    "cullpdb_loader.print_residues(train_x[0], labels=train_y[0], two_d=TWO_D)\n",
    "\n",
    "train_y = train_y.reshape(len(train_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "test_y = test_y.reshape(len(test_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "\n",
    "\n",
    "# print to verify data was loaded in correct shapes:\n",
    "print(\"train_x:\", train_x.shape)\n",
    "print(\"train_y\", train_y.shape)\n",
    "#print(vali_x.shape)\n",
    "#print(vali_y.shape)\n",
    "print(\"test_x:\", test_x.shape)\n",
    "print(\"test_y:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accuracy Metric (Test)\n",
    "\n",
    "Currently, if inputs and outputs are both 2D (samples, protein length, num residues/classes) and the final layer of model is Reshape (from flattened to 2D), then Keras does not compute correct loss (wrong default axis maybe?). If inputs and outputs are both 1D (samples, length * residues), then Keras calculates loss correctly (sort of) but cannot compute accuracy (no notion of separate class one-hot vectors since they are all flattened).\n",
    "<br><br>\n",
    "Working fix: Omit Reshape layer, use default Keras loss, reshape labels to 1D, and define custom accuracy to reshape outputs to 2D before comparing to predicted labels.\n",
    "<br><br>\n",
    "TODO: Define custom Loss function to calculate correct loss on 2d inputs/outputs, use default accuracy, add Reshape layer to end of model, and don't have to reshape any inputs or labels manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# old metric - accuracy inflated with trailing 'NoSeq' equality\n",
    "def custom_acc_old(obs, pred):\n",
    "    # -1 = placeholder for whatever's left\n",
    "    obs1 = K.reshape(obs, [-1, 700, 9])\n",
    "    pred1 = K.reshape(pred, [-1, 700, 9])\n",
    "    return K.mean(K.cast(\n",
    "            K.equal(K.argmax(obs1, axis=2), K.argmax(pred1, axis=2)),\n",
    "            dtype=K.floatx()))\n",
    "    \n",
    "# Second custom accuracy: ignore trailing noseq's\n",
    "def custom_acc(true, obs):\n",
    "    print(\"Using custom accuracy\")\n",
    "    if not SHOW_ACCURACY:\n",
    "        return K.constant(float('NaN'))\n",
    "    \n",
    "    # -1 = placeholder for whatever's left\n",
    "    obs = K.reshape(obs, [-1, 700, 9])\n",
    "    true = K.reshape(true, [-1, 700, 9])\n",
    "    \n",
    "    # convert one-hot vectors for residues to scalars\n",
    "    true_vals = K.argmax(true, axis=2)\n",
    "    obs_vals = K.argmax(obs, axis=2)\n",
    "    \n",
    "    # mask is 2D matrix with 1s in indices that are residues\n",
    "    # and 0s in indices that are 'NoSeq'\n",
    "    # subtract all 8's to shift 'NoSeq' values to zero\n",
    "    mask = K.sign(K.abs(true_vals - 8*K.ones_like(true_vals, dtype='int64')))\n",
    "    mask = K.cast(mask, dtype=K.floatx())\n",
    "    # 1D vector with each index the number of non-'NoSeq' residues \n",
    "    # in corresponding protein\n",
    "    length = K.sum(mask, axis=1)\n",
    "    \n",
    "    # compare observed and predicted values (cast from boolean to 1s and 0s),\n",
    "    # then multiply by mask to nullify any trailing 'NoSeq' equalities\n",
    "    comparison = K.cast(K.equal(true_vals, obs_vals), dtype=K.floatx())\n",
    "    comparison = comparison * mask\n",
    "    \n",
    "    # and return average\n",
    "    return K.sum(comparison) / K.sum(length)\n",
    "    \n",
    "\n",
    "# not working\n",
    "def custom_loss(obs, pred):\n",
    "    pred = tf.Variable(pred)\n",
    "    pred.set_shape([None, NUM_RESIDUES, NUM_CLASSES])\n",
    "    obs = tf.Variable(obs)\n",
    "    obs.set_shape([None, NUM_RESIDUES, NUM_CLASSES])\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=pred, logits=obs, name='custom-loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "The model is constructed to currently have 3 convolutional layers.<br><br>\n",
    "A convolutional window \"slides\" across the input, each time taking the nearest <i>WINDOW_SIZE</i> number of features (residues) as inputs and outputing a single vector of dimension <i>FILTERS</i>.\n",
    "<br><br>\n",
    "This 3d data (samples, Protein Length, <i>FILTER_SIZE</i>) is flattened to 2d (samples, Length \\* <i>FILTER_SIZE</i>). Finally a \"dense\", fully connected layer reduces the dimensionality of the data from the previous <i>FILTERS</i> dimensions to <i>OUTPUT_SIZE</i> dimensions (here 9, 8 for each secondary structure class and 1 for None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# border_mode indicates how the convolution handles the edges of the input (where the window \"sticks out\").\n",
    "# The 'same' setting pads the inputs with zeros on either side.\n",
    "# Only the first layer requires an input_shape parameter; successive layers infer it from within the model.\n",
    "\n",
    "# 1st layer\n",
    "model.add(Convolution1D(\n",
    "        FILTERS, WINDOW_SIZE, activation='tanh', border_mode='same', input_shape=INPUT_SHAPE))\n",
    "# 2nd layer\n",
    "model.add(Convolution1D(\n",
    "        FILTERS, WINDOW_SIZE, activation='tanh', border_mode='same'))\n",
    "# 3rd layer\n",
    "#model.add(Convolution1D(\n",
    "#        FILTERS, WINDOW_SIZE, activation='tanh', border_mode='same'))\n",
    "\n",
    "# Experiment - Dense final layer\n",
    "# 'lecun_uniform' indicates that the weights should be initialized to small random values in a certain normal distribution.\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(OUTPUT_SIZE, init='lecun_uniform', activation='tanh')\n",
    "model.add(Dense(OUTPUT_SIZE, init='lecun_uniform', name=\"OutputLayer\", activation='softmax'))\n",
    "#model.add(Reshape((NUM_RESIDUES, NUM_CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model given a loss function, optimizer, and learning rate (specified above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom accuracy\n"
     ]
    }
   ],
   "source": [
    "# optimizer= takes either string or optimizer object\n",
    "\n",
    "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[custom_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on training data against target training labels, show accuracy on validation data each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5534 samples, validate on 514 samples\n",
      "Epoch 1/30\n",
      "18s - loss: 5187.7910 - custom_acc: 0.1910 - val_loss: 5027.4035 - val_custom_acc: 0.2552\n",
      "Epoch 2/30\n",
      "19s - loss: 5061.9217 - custom_acc: 0.2596 - val_loss: 4972.4117 - val_custom_acc: 0.2838\n",
      "Epoch 3/30\n",
      "19s - loss: 5029.5167 - custom_acc: 0.2934 - val_loss: 4945.7162 - val_custom_acc: 0.2891\n",
      "Epoch 4/30\n",
      "19s - loss: 5010.4700 - custom_acc: 0.3097 - val_loss: 4931.1062 - val_custom_acc: 0.2899\n",
      "Epoch 5/30\n",
      "20s - loss: 4998.7691 - custom_acc: 0.3189 - val_loss: 4922.5865 - val_custom_acc: 0.2925\n",
      "Epoch 6/30\n",
      "20s - loss: 4990.7339 - custom_acc: 0.3250 - val_loss: 4916.5779 - val_custom_acc: 0.2952\n",
      "Epoch 7/30\n",
      "19s - loss: 4984.5881 - custom_acc: 0.3294 - val_loss: 4911.9522 - val_custom_acc: 0.2973\n",
      "Epoch 8/30\n",
      "19s - loss: 4979.6766 - custom_acc: 0.3326 - val_loss: 4908.2224 - val_custom_acc: 0.2992\n",
      "Epoch 9/30\n",
      "19s - loss: 4975.6937 - custom_acc: 0.3353 - val_loss: 4905.1455 - val_custom_acc: 0.3010\n",
      "Epoch 10/30\n",
      "19s - loss: 4972.3267 - custom_acc: 0.3375 - val_loss: 4902.5515 - val_custom_acc: 0.3025\n",
      "Epoch 11/30\n",
      "19s - loss: 4969.4637 - custom_acc: 0.3393 - val_loss: 4900.3336 - val_custom_acc: 0.3035\n",
      "Epoch 12/30\n",
      "19s - loss: 4966.9831 - custom_acc: 0.3409 - val_loss: 4898.4102 - val_custom_acc: 0.3046\n",
      "Epoch 13/30\n",
      "19s - loss: 4964.8244 - custom_acc: 0.3422 - val_loss: 4896.7273 - val_custom_acc: 0.3051\n",
      "Epoch 14/30\n",
      "20s - loss: 4962.9207 - custom_acc: 0.3432 - val_loss: 4895.2436 - val_custom_acc: 0.3058\n",
      "Epoch 15/30\n",
      "20s - loss: 4961.2319 - custom_acc: 0.3443 - val_loss: 4893.9249 - val_custom_acc: 0.3063\n",
      "Epoch 16/30\n",
      "19s - loss: 4959.7225 - custom_acc: 0.3453 - val_loss: 4892.7440 - val_custom_acc: 0.3069\n",
      "Epoch 17/30\n",
      "19s - loss: 4958.3643 - custom_acc: 0.3461 - val_loss: 4891.6805 - val_custom_acc: 0.3072\n",
      "Epoch 18/30\n",
      "19s - loss: 4957.1348 - custom_acc: 0.3469 - val_loss: 4890.7160 - val_custom_acc: 0.3078\n",
      "Epoch 19/30\n",
      "19s - loss: 4956.0137 - custom_acc: 0.3477 - val_loss: 4889.8375 - val_custom_acc: 0.3079\n",
      "Epoch 20/30\n",
      "19s - loss: 4954.9867 - custom_acc: 0.3485 - val_loss: 4889.0343 - val_custom_acc: 0.3082\n",
      "Epoch 21/30\n",
      "19s - loss: 4954.0424 - custom_acc: 0.3491 - val_loss: 4888.2963 - val_custom_acc: 0.3084\n",
      "Epoch 22/30\n",
      "19s - loss: 4953.1693 - custom_acc: 0.3497 - val_loss: 4887.6156 - val_custom_acc: 0.3089\n",
      "Epoch 23/30\n",
      "19s - loss: 4952.3589 - custom_acc: 0.3502 - val_loss: 4886.9854 - val_custom_acc: 0.3091\n",
      "Epoch 24/30\n",
      "19s - loss: 4951.6044 - custom_acc: 0.3507 - val_loss: 4886.4002 - val_custom_acc: 0.3093\n",
      "Epoch 25/30\n",
      "20s - loss: 4950.8997 - custom_acc: 0.3512 - val_loss: 4885.8548 - val_custom_acc: 0.3095\n",
      "Epoch 26/30\n",
      "19s - loss: 4950.2394 - custom_acc: 0.3517 - val_loss: 4885.3445 - val_custom_acc: 0.3096\n",
      "Epoch 27/30\n",
      "20s - loss: 4949.6193 - custom_acc: 0.3522 - val_loss: 4884.8654 - val_custom_acc: 0.3099\n",
      "Epoch 28/30\n",
      "19s - loss: 4949.0348 - custom_acc: 0.3527 - val_loss: 4884.4143 - val_custom_acc: 0.3105\n",
      "Epoch 29/30\n",
      "19s - loss: 4948.4823 - custom_acc: 0.3531 - val_loss: 4883.9874 - val_custom_acc: 0.3109\n",
      "Epoch 30/30\n",
      "19s - loss: 4947.9586 - custom_acc: 0.3535 - val_loss: 4883.5824 - val_custom_acc: 0.3110\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "# verbose: 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "hist = model.fit(train_x, train_y, nb_epoch=EPOCHS, shuffle=False, verbose=2,\n",
    "                 validation_data=(test_x, test_y))\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAVE_FILE = \"models/conv1dense-filtered-3-28\"\n",
    "model.save(SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5f22bf611c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvali_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvali_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "vali_loss = list(hist.history.values())[1]\n",
    "plt.plot(range(EPOCHS), vali_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on cb513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(test_x, test_y)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "prediction = model.predict(test_x[i:i+1])\n",
    "print(\"Shape:\", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x1 = train_x.reshape(len(train_x), 700*22)\n",
    "train_y1 = train_y.reshape(len(train_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "prediction1 = prediction.reshape(len(prediction), 700*9)\n",
    "test_x1 = test_x.reshape(len(test_x), RESIDUE_SIZE*NUM_RESIDUES)\n",
    "test_y1 = test_y.reshape(len(test_y), NUM_CLASSES*NUM_RESIDUES)\n",
    "print(train_y.shape)\n",
    "print(prediction.shape)\n",
    "#TWO_D = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Expected:\\n\")\n",
    "_ = cullpdb_loader.print_residues(test_x[i], labels=test_y1[i], two_d=True)\n",
    "print(\"\\nPredicted:\\n\")\n",
    "_ = cullpdb_loader.print_residues(test_x[i], labels=prediction1[0], two_d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdscbio]",
   "language": "python",
   "name": "conda-env-sdscbio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
